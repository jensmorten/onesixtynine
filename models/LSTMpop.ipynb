{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b8340f-6c73-4038-9464-6afafda79fdc",
   "metadata": {
    "id": "f4b8340f-6c73-4038-9464-6afafda79fdc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense, LayerNormalization, Dropout, Lambda\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938fa0af-d0a3-48e4-b5d1-6051afe64437",
   "metadata": {
    "id": "938fa0af-d0a3-48e4-b5d1-6051afe64437"
   },
   "outputs": [],
   "source": [
    "# Laste poll-of-polls data\n",
    "url = \"https://raw.githubusercontent.com/jensmorten/onesixtynine/main/data/pollofpolls_master.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee248ca9-0fa2-44f0-a08f-ebaf3dc6b24f",
   "metadata": {
    "id": "ee248ca9-0fa2-44f0-a08f-ebaf3dc6b24f"
   },
   "outputs": [],
   "source": [
    "# Convert to datetime and set the date to the end of the month\n",
    "df[\"Mnd\"] = pd.to_datetime(df[\"Mnd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d222447-b7e1-4ec4-989a-8728f2946afd",
   "metadata": {
    "id": "2d222447-b7e1-4ec4-989a-8728f2946afd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2008-01-31', '2008-02-29', '2008-03-31', '2008-04-30',\n",
       "               '2008-05-31', '2008-06-30', '2008-07-31', '2008-08-31',\n",
       "               '2008-09-30', '2008-10-31',\n",
       "               ...\n",
       "               '2025-01-31', '2025-02-28', '2025-03-31', '2025-04-30',\n",
       "               '2025-05-31', '2025-06-30', '2025-07-31', '2025-08-31',\n",
       "               '2025-09-30', '2025-10-31'],\n",
       "              dtype='datetime64[ns]', name='Mnd', length=214, freq='ME')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort values and set index\n",
    "df = df.sort_values(\"Mnd\")\n",
    "df.set_index(\"Mnd\", inplace=True)\n",
    "df.index.to_period('M').to_timestamp('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ed3098-74c1-4dbf-88e0-730ef77b723f",
   "metadata": {
    "id": "05ed3098-74c1-4dbf-88e0-730ef77b723f"
   },
   "outputs": [],
   "source": [
    "df_en=df[[\"Ap\",\"Hoyre\",\"Frp\",\"SV\",\"SP\",\"KrF\",\"Venstre\",\"MDG\",\"Rodt\", \"Andre\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "542d8b5f-cdb1-4c39-84f7-7ab378cc8d42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "542d8b5f-cdb1-4c39-84f7-7ab378cc8d42",
    "outputId": "3abcd4f6-321e-47e7-fc60-cd8b02662fc6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ap</th>\n",
       "      <th>Hoyre</th>\n",
       "      <th>Frp</th>\n",
       "      <th>SV</th>\n",
       "      <th>SP</th>\n",
       "      <th>KrF</th>\n",
       "      <th>Venstre</th>\n",
       "      <th>MDG</th>\n",
       "      <th>Rodt</th>\n",
       "      <th>Andre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mnd</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-01-31</th>\n",
       "      <td>29.3</td>\n",
       "      <td>17.2</td>\n",
       "      <td>23.9</td>\n",
       "      <td>7.4</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.4</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-29</th>\n",
       "      <td>29.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>25.2</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-03-31</th>\n",
       "      <td>28.7</td>\n",
       "      <td>18.1</td>\n",
       "      <td>25.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-04-30</th>\n",
       "      <td>29.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-05-31</th>\n",
       "      <td>28.9</td>\n",
       "      <td>17.8</td>\n",
       "      <td>25.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-30</th>\n",
       "      <td>28.3</td>\n",
       "      <td>16.2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-31</th>\n",
       "      <td>27.7</td>\n",
       "      <td>14.8</td>\n",
       "      <td>21.5</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.3</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-31</th>\n",
       "      <td>27.3</td>\n",
       "      <td>15.3</td>\n",
       "      <td>21.2</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.3</td>\n",
       "      <td>6.1</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-30</th>\n",
       "      <td>27.1</td>\n",
       "      <td>14.4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-31</th>\n",
       "      <td>27.8</td>\n",
       "      <td>15.3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Ap  Hoyre   Frp   SV   SP  KrF  Venstre  MDG  Rodt  Andre\n",
       "Mnd                                                                    \n",
       "2008-01-31  29.3   17.2  23.9  7.4  6.1  6.4      6.5  0.0   1.3    0.0\n",
       "2008-02-29  29.0   17.3  25.2  6.7  5.9  6.3      6.6  0.0   1.3    0.0\n",
       "2008-03-31  28.7   18.1  25.2  7.0  5.6  6.1      6.3  0.0   1.1    0.0\n",
       "2008-04-30  29.0   16.9  25.4  6.5  5.5  7.0      6.8  0.0   1.2    0.0\n",
       "2008-05-31  28.9   17.8  25.9  6.7  5.7  6.2      6.2  0.0   1.4    0.0\n",
       "...          ...    ...   ...  ...  ...  ...      ...  ...   ...    ...\n",
       "2025-06-30  28.3   16.2  21.0  6.9  5.6  3.7      4.5  3.0   6.2    4.5\n",
       "2025-07-31  27.7   14.8  21.5  8.2  6.3  3.2      4.7  3.5   5.9    4.1\n",
       "2025-08-31  27.3   15.3  21.2  6.3  6.2  4.6      4.2  4.3   6.1    4.5\n",
       "2025-09-30  27.1   14.4  21.0  6.0  5.9  4.7      4.3  6.2   6.0    4.4\n",
       "2025-10-31  27.8   15.3  24.0  5.8  5.2  4.2      3.8  4.5   6.2    4.0\n",
       "\n",
       "[214 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af0723d5-f132-471e-a368-940f73bf01b0",
   "metadata": {
    "id": "af0723d5-f132-471e-a368-940f73bf01b0"
   },
   "outputs": [],
   "source": [
    "#n_timesteps = 5   # past steps to look at\n",
    "#n_future = 12      # steps into the future we want to predict\n",
    "n_features = df_en.shape[1]\n",
    "series_names = df_en.columns\n",
    "window_size=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98d672de-30c1-439f-96d9-4225d690fcf8",
   "metadata": {
    "id": "98d672de-30c1-439f-96d9-4225d690fcf8"
   },
   "outputs": [],
   "source": [
    "def windowed_dataset_multivariate(series, window_size, batch_size, shuffle_buffer):\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset for multivariate time series.\n",
    "\n",
    "    series: numpy array or tf.Tensor with shape (num_timesteps, num_features)\n",
    "    window_size: number of timesteps in the input window\n",
    "    batch_size: training batch size\n",
    "    shuffle_buffer: buffer size for shuffling\n",
    "    \"\"\"\n",
    "    # Make a Dataset of timesteps\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "\n",
    "    # Create sliding windows of length (window_size + 1)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "\n",
    "    # Convert each window into a batch tensor\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "\n",
    "    # Shuffle windows\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "\n",
    "    # Split into (input, label):\n",
    "    # inputs = first window_size steps, labels = last step\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "\n",
    "    # Batch and prefetch for performance\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e910c3-9404-47f6-9db4-a27587d04cbb",
   "metadata": {
    "id": "c0e910c3-9404-47f6-9db4-a27587d04cbb"
   },
   "outputs": [],
   "source": [
    "#split = int(len(X) * 0.90)\n",
    "#X_train, X_test = X[:split], X[split:]\n",
    "#y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90a5ff6d-6ba7-4334-90b6-a56147f8fbdb",
   "metadata": {
    "id": "90a5ff6d-6ba7-4334-90b6-a56147f8fbdb"
   },
   "outputs": [],
   "source": [
    "window_size = 12\n",
    "batch_size = 100\n",
    "shuffle_buffer = 100\n",
    "\n",
    "split = int(len(df_en) * 0.90)\n",
    "train_data = df_en.values[:split]\n",
    "val_data   = df_en.values[split:]\n",
    "\n",
    "train_dataset = windowed_dataset_multivariate(train_data, window_size, batch_size, shuffle_buffer=shuffle_buffer)\n",
    "val_dataset   = windowed_dataset_multivariate(val_data,   window_size, batch_size, shuffle_buffer=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7a06b87-0f8f-4130-8de9-8e14151652fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7a06b87-0f8f-4130-8de9-8e14151652fa",
    "outputId": "a4925239-c8e0-44ce-d478-c9b29ce61437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column weights: [0.01663109 0.01887045 0.03161702 0.07858111 0.05805144 0.10540169\n",
      " 0.11078599 0.1764926  0.1628699  0.24069871]\n"
     ]
    }
   ],
   "source": [
    "y_values = []\n",
    "for _, y in train_dataset:\n",
    "    y_values.append(y.numpy())\n",
    "\n",
    "y_all = np.concatenate(y_values, axis=0)  # shape: (num_train, num_features)\n",
    "\n",
    "# Compute column weights (inverse of std)\n",
    "col_mean = np.mean(y_all, axis=0)\n",
    "power = 1  # increase emphasis on small parties\n",
    "col_weights = (1.0 / (col_mean)) ** power\n",
    "##col_weights = 1.0 / (col_stds + 1e-6)  # avoid divide-by-zero\n",
    "col_weights = col_weights / np.sum(col_weights)  # normalize to mean 1\n",
    "\n",
    "print(\"Column weights:\", col_weights)\n",
    "col_weights_tf = tf.constant(col_weights, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7067c0fd-11b7-4bee-ac44-18570a461a2a",
   "metadata": {
    "id": "7067c0fd-11b7-4bee-ac44-18570a461a2a"
   },
   "outputs": [],
   "source": [
    "def weighted_mse(y_true, y_pred):\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    weighted_error = squared_error * col_weights_tf  # broadcast along features\n",
    "    return tf.reduce_mean(weighted_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0a6bcd2-3afe-4820-b5ee-f65f0c982d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_mae(y_true, y_pred):\n",
    "    abs_error = tf.abs(y_true - y_pred)/(abs(y_pred+1E-3))\n",
    "    weighted_error = abs_error*100 ##* col_weights_tf  # broadcast along features\n",
    "    return tf.reduce_mean(weighted_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ca5aaa7-03bb-485a-9799-87526c698d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jens.nilsen\\python\\WPy64-31230\\python-3.12.3.amd64\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">840</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m840\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m110\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">950</span> (3.71 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m950\u001b[0m (3.71 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">950</span> (3.71 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m950\u001b[0m (3.71 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_features = 10\n",
    "model = Sequential([ \n",
    "    tf.keras.layers.Input(shape=(window_size, n_features)), \n",
    "    #tf.keras.layers.Conv1D(filters=10, \n",
    "    #                       kernel_size=window_size, \n",
    "    #                       strides=int(np.floor(window_size/(10))), \n",
    "    #                       activation=\"relu\", \n",
    "    #                       padding='causal'), \n",
    "    tf.keras.layers.LSTM(10, return_sequences=False), \n",
    "    tf.keras.layers.Dense(n_features),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.nn.softmax(x) * 100) \n",
    "])\n",
    "\n",
    "model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), \n",
    "              loss=relative_mae, # or \"mae\" if you prefer absolute error\n",
    "              metrics=[\"mae\"] )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f39c2f-c82b-491e-b469-dd1d7a7a43b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2f39c2f-c82b-491e-b469-dd1d7a7a43b4",
    "outputId": "3965dad3-df8d-47f7-bb3b-140608e3c39a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "      1/Unknown \u001b[1m3s\u001b[0m 3s/step - loss: 94.5754 - mae: 8.3007"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jens.nilsen\\python\\WPy64-31230\\python-3.12.3.amd64\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 468ms/step - loss: 94.0451 - mae: 8.2681 - val_loss: 71.9998 - val_mae: 6.4293 - learning_rate: 1.0000e-05\n",
      "Epoch 2/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 94.0294 - mae: 8.2674 - val_loss: 71.9847 - val_mae: 6.4283 - learning_rate: 1.0000e-05\n",
      "Epoch 3/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 94.0152 - mae: 8.2667 - val_loss: 71.9693 - val_mae: 6.4273 - learning_rate: 1.0000e-05\n",
      "Epoch 4/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 93.9992 - mae: 8.2660 - val_loss: 71.9534 - val_mae: 6.4262 - learning_rate: 1.0000e-05\n",
      "Epoch 5/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 93.9839 - mae: 8.2653 - val_loss: 71.9371 - val_mae: 6.4252 - learning_rate: 1.0000e-05\n",
      "Epoch 6/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 93.9684 - mae: 8.2646 - val_loss: 71.9203 - val_mae: 6.4240 - learning_rate: 1.0000e-05\n",
      "Epoch 7/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 93.9521 - mae: 8.2639 - val_loss: 71.9030 - val_mae: 6.4229 - learning_rate: 1.0000e-05\n",
      "Epoch 8/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 93.9357 - mae: 8.2631 - val_loss: 71.8854 - val_mae: 6.4217 - learning_rate: 1.0000e-05\n",
      "Epoch 9/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 93.9184 - mae: 8.2623 - val_loss: 71.8673 - val_mae: 6.4205 - learning_rate: 1.0000e-05\n",
      "Epoch 10/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 93.9006 - mae: 8.2615 - val_loss: 71.8490 - val_mae: 6.4193 - learning_rate: 1.0000e-05\n",
      "Epoch 11/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 93.8830 - mae: 8.2607 - val_loss: 71.8304 - val_mae: 6.4181 - learning_rate: 1.0000e-05\n",
      "Epoch 12/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 93.8632 - mae: 8.2599 - val_loss: 71.8116 - val_mae: 6.4168 - learning_rate: 1.0000e-05\n",
      "Epoch 13/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 93.8465 - mae: 8.2591 - val_loss: 71.7924 - val_mae: 6.4155 - learning_rate: 1.0000e-05\n",
      "Epoch 14/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 93.8258 - mae: 8.2582 - val_loss: 71.7730 - val_mae: 6.4142 - learning_rate: 1.0000e-05\n",
      "Epoch 15/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 93.8073 - mae: 8.2574 - val_loss: 71.7535 - val_mae: 6.4129 - learning_rate: 1.0000e-05\n",
      "Epoch 16/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 93.7867 - mae: 8.2566 - val_loss: 71.7339 - val_mae: 6.4116 - learning_rate: 1.0000e-05\n",
      "Epoch 17/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 93.7668 - mae: 8.2557 - val_loss: 71.7140 - val_mae: 6.4103 - learning_rate: 1.0000e-05\n",
      "Epoch 18/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 93.7472 - mae: 8.2548 - val_loss: 71.6941 - val_mae: 6.4090 - learning_rate: 1.0000e-05\n",
      "Epoch 19/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 93.7273 - mae: 8.2540 - val_loss: 71.6741 - val_mae: 6.4076 - learning_rate: 1.0000e-05\n",
      "Epoch 20/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 93.7063 - mae: 8.2531 - val_loss: 71.6540 - val_mae: 6.4063 - learning_rate: 1.0000e-05\n",
      "Epoch 21/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 93.6849 - mae: 8.2522 - val_loss: 71.6338 - val_mae: 6.4049 - learning_rate: 1.0000e-05\n",
      "Epoch 22/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 93.6631 - mae: 8.2513 - val_loss: 71.6136 - val_mae: 6.4036 - learning_rate: 1.0000e-05\n",
      "Epoch 23/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 93.6426 - mae: 8.2504 - val_loss: 71.5934 - val_mae: 6.4022 - learning_rate: 1.0000e-05\n",
      "Epoch 24/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 93.6194 - mae: 8.2495 - val_loss: 71.5731 - val_mae: 6.4009 - learning_rate: 1.0000e-05\n",
      "Epoch 25/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 93.5991 - mae: 8.2486 - val_loss: 71.5529 - val_mae: 6.3995 - learning_rate: 1.0000e-05\n",
      "Epoch 26/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 93.5758 - mae: 8.2476 - val_loss: 71.5326 - val_mae: 6.3982 - learning_rate: 1.0000e-05\n",
      "Epoch 27/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 93.5531 - mae: 8.2467 - val_loss: 71.5121 - val_mae: 6.3968 - learning_rate: 1.0000e-05\n",
      "Epoch 28/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 93.5325 - mae: 8.2458 - val_loss: 71.4916 - val_mae: 6.3954 - learning_rate: 1.0000e-05\n",
      "Epoch 29/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 93.5095 - mae: 8.2449 - val_loss: 71.4710 - val_mae: 6.3941 - learning_rate: 1.0000e-05\n",
      "Epoch 30/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 93.4855 - mae: 8.2439 - val_loss: 71.4504 - val_mae: 6.3927 - learning_rate: 1.0000e-05\n",
      "Epoch 31/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 93.4622 - mae: 8.2430 - val_loss: 71.4301 - val_mae: 6.3913 - learning_rate: 1.0000e-05\n",
      "Epoch 32/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 93.4400 - mae: 8.2421 - val_loss: 71.4095 - val_mae: 6.3899 - learning_rate: 1.0000e-05\n",
      "Epoch 33/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 93.4161 - mae: 8.2411 - val_loss: 71.3888 - val_mae: 6.3885 - learning_rate: 1.0000e-05\n",
      "Epoch 34/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 93.3930 - mae: 8.2401 - val_loss: 71.3682 - val_mae: 6.3872 - learning_rate: 1.0000e-05\n",
      "Epoch 35/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 93.3670 - mae: 8.2391 - val_loss: 71.3476 - val_mae: 6.3858 - learning_rate: 1.0000e-05\n",
      "Epoch 36/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 93.3454 - mae: 8.2382 - val_loss: 71.3270 - val_mae: 6.3844 - learning_rate: 1.0000e-05\n",
      "Epoch 37/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 93.3192 - mae: 8.2372 - val_loss: 71.3063 - val_mae: 6.3830 - learning_rate: 1.0000e-05\n",
      "Epoch 38/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 93.2978 - mae: 8.2362 - val_loss: 71.2855 - val_mae: 6.3816 - learning_rate: 1.0000e-05\n",
      "Epoch 39/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 93.2728 - mae: 8.2353 - val_loss: 71.2649 - val_mae: 6.3802 - learning_rate: 1.0000e-05\n",
      "Epoch 40/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 93.2478 - mae: 8.2342 - val_loss: 71.2442 - val_mae: 6.3788 - learning_rate: 1.0000e-05\n",
      "Epoch 41/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 93.2229 - mae: 8.2332 - val_loss: 71.2234 - val_mae: 6.3774 - learning_rate: 1.0000e-05\n",
      "Epoch 42/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 93.1984 - mae: 8.2322 - val_loss: 71.2026 - val_mae: 6.3760 - learning_rate: 1.0000e-05\n",
      "Epoch 43/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 93.1718 - mae: 8.2312 - val_loss: 71.1818 - val_mae: 6.3746 - learning_rate: 1.0000e-05\n",
      "Epoch 44/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 93.1489 - mae: 8.2302 - val_loss: 71.1609 - val_mae: 6.3732 - learning_rate: 1.0000e-05\n",
      "Epoch 45/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 93.1241 - mae: 8.2293 - val_loss: 71.1400 - val_mae: 6.3718 - learning_rate: 1.0000e-05\n",
      "Epoch 46/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 93.0998 - mae: 8.2283 - val_loss: 71.1190 - val_mae: 6.3704 - learning_rate: 1.0000e-05\n",
      "Epoch 47/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 93.0732 - mae: 8.2273 - val_loss: 71.0980 - val_mae: 6.3690 - learning_rate: 1.0000e-05\n",
      "Epoch 48/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 93.0510 - mae: 8.2264 - val_loss: 71.0769 - val_mae: 6.3676 - learning_rate: 1.0000e-05\n",
      "Epoch 49/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 93.0273 - mae: 8.2255 - val_loss: 71.0558 - val_mae: 6.3661 - learning_rate: 1.0000e-05\n",
      "Epoch 50/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 93.0007 - mae: 8.2245 - val_loss: 71.0347 - val_mae: 6.3647 - learning_rate: 1.0000e-05\n",
      "Epoch 51/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 92.9775 - mae: 8.2235 - val_loss: 71.0135 - val_mae: 6.3633 - learning_rate: 1.0000e-05\n",
      "Epoch 52/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 92.9552 - mae: 8.2226 - val_loss: 70.9923 - val_mae: 6.3618 - learning_rate: 1.0000e-05\n",
      "Epoch 53/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 92.9316 - mae: 8.2218 - val_loss: 70.9712 - val_mae: 6.3604 - learning_rate: 1.0000e-05\n",
      "Epoch 54/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 92.9085 - mae: 8.2209 - val_loss: 70.9501 - val_mae: 6.3590 - learning_rate: 1.0000e-05\n",
      "Epoch 55/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 92.8848 - mae: 8.2200 - val_loss: 70.9289 - val_mae: 6.3575 - learning_rate: 1.0000e-05\n",
      "Epoch 56/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 92.8619 - mae: 8.2191 - val_loss: 70.9077 - val_mae: 6.3561 - learning_rate: 1.0000e-05\n",
      "Epoch 57/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 92.8402 - mae: 8.2183 - val_loss: 70.8866 - val_mae: 6.3547 - learning_rate: 1.0000e-05\n",
      "Epoch 58/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 92.8166 - mae: 8.2174 - val_loss: 70.8653 - val_mae: 6.3532 - learning_rate: 1.0000e-05\n",
      "Epoch 59/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 92.7947 - mae: 8.2165 - val_loss: 70.8441 - val_mae: 6.3518 - learning_rate: 1.0000e-05\n",
      "Epoch 60/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 92.7710 - mae: 8.2156 - val_loss: 70.8229 - val_mae: 6.3504 - learning_rate: 1.0000e-05\n",
      "Epoch 61/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 92.7509 - mae: 8.2147 - val_loss: 70.8015 - val_mae: 6.3489 - learning_rate: 1.0000e-05\n",
      "Epoch 62/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 92.7268 - mae: 8.2137 - val_loss: 70.7802 - val_mae: 6.3475 - learning_rate: 1.0000e-05\n",
      "Epoch 63/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 92.7063 - mae: 8.2129 - val_loss: 70.7589 - val_mae: 6.3460 - learning_rate: 1.0000e-05\n",
      "Epoch 64/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 92.6827 - mae: 8.2119 - val_loss: 70.7374 - val_mae: 6.3445 - learning_rate: 1.0000e-05\n",
      "Epoch 65/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 92.6597 - mae: 8.2111 - val_loss: 70.7161 - val_mae: 6.3431 - learning_rate: 1.0000e-05\n",
      "Epoch 66/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 92.6408 - mae: 8.2102 - val_loss: 70.6947 - val_mae: 6.3416 - learning_rate: 1.0000e-05\n",
      "Epoch 67/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 92.6185 - mae: 8.2094 - val_loss: 70.6733 - val_mae: 6.3402 - learning_rate: 1.0000e-05\n",
      "Epoch 68/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 92.5984 - mae: 8.2086 - val_loss: 70.6520 - val_mae: 6.3387 - learning_rate: 1.0000e-05\n",
      "Epoch 69/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 92.5784 - mae: 8.2077 - val_loss: 70.6306 - val_mae: 6.3373 - learning_rate: 1.0000e-05\n",
      "Epoch 70/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 92.5563 - mae: 8.2069 - val_loss: 70.6092 - val_mae: 6.3358 - learning_rate: 1.0000e-05\n",
      "Epoch 71/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 92.5363 - mae: 8.2060 - val_loss: 70.5877 - val_mae: 6.3344 - learning_rate: 1.0000e-05\n",
      "Epoch 72/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 92.5165 - mae: 8.2052 - val_loss: 70.5662 - val_mae: 6.3329 - learning_rate: 1.0000e-05\n",
      "Epoch 73/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 92.4955 - mae: 8.2043 - val_loss: 70.5448 - val_mae: 6.3314 - learning_rate: 1.0000e-05\n",
      "Epoch 74/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 92.4761 - mae: 8.2034 - val_loss: 70.5232 - val_mae: 6.3300 - learning_rate: 1.0000e-05\n",
      "Epoch 75/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 92.4553 - mae: 8.2026 - val_loss: 70.5016 - val_mae: 6.3285 - learning_rate: 1.0000e-05\n",
      "Epoch 76/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 92.4355 - mae: 8.2017 - val_loss: 70.4800 - val_mae: 6.3270 - learning_rate: 1.0000e-05\n",
      "Epoch 77/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 92.4150 - mae: 8.2008 - val_loss: 70.4585 - val_mae: 6.3255 - learning_rate: 1.0000e-05\n",
      "Epoch 78/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 92.3948 - mae: 8.2000 - val_loss: 70.4370 - val_mae: 6.3241 - learning_rate: 1.0000e-05\n",
      "Epoch 79/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 92.3761 - mae: 8.1991 - val_loss: 70.4154 - val_mae: 6.3226 - learning_rate: 1.0000e-05\n",
      "Epoch 80/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 92.3562 - mae: 8.1982 - val_loss: 70.3937 - val_mae: 6.3211 - learning_rate: 1.0000e-05\n",
      "Epoch 81/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 92.3354 - mae: 8.1973 - val_loss: 70.3721 - val_mae: 6.3196 - learning_rate: 1.0000e-05\n",
      "Epoch 82/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 92.3169 - mae: 8.1965 - val_loss: 70.3504 - val_mae: 6.3181 - learning_rate: 1.0000e-05\n",
      "Epoch 83/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 92.2971 - mae: 8.1956 - val_loss: 70.3288 - val_mae: 6.3167 - learning_rate: 1.0000e-05\n",
      "Epoch 84/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 92.2782 - mae: 8.1947 - val_loss: 70.3071 - val_mae: 6.3152 - learning_rate: 1.0000e-05\n",
      "Epoch 85/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 92.2574 - mae: 8.1938 - val_loss: 70.2854 - val_mae: 6.3137 - learning_rate: 1.0000e-05\n",
      "Epoch 86/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 92.2380 - mae: 8.1929 - val_loss: 70.2637 - val_mae: 6.3122 - learning_rate: 1.0000e-05\n",
      "Epoch 87/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 92.2192 - mae: 8.1919 - val_loss: 70.2421 - val_mae: 6.3107 - learning_rate: 1.0000e-05\n",
      "Epoch 88/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 92.2007 - mae: 8.1911 - val_loss: 70.2203 - val_mae: 6.3092 - learning_rate: 1.0000e-05\n",
      "Epoch 89/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 92.1808 - mae: 8.1902 - val_loss: 70.1985 - val_mae: 6.3077 - learning_rate: 1.0000e-05\n",
      "Epoch 90/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 92.1622 - mae: 8.1893 - val_loss: 70.1767 - val_mae: 6.3062 - learning_rate: 1.0000e-05\n",
      "Epoch 91/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 92.1427 - mae: 8.1883 - val_loss: 70.1550 - val_mae: 6.3047 - learning_rate: 1.0000e-05\n",
      "Epoch 92/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 92.1244 - mae: 8.1874 - val_loss: 70.1332 - val_mae: 6.3032 - learning_rate: 1.0000e-05\n",
      "Epoch 93/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 92.1042 - mae: 8.1865 - val_loss: 70.1114 - val_mae: 6.3017 - learning_rate: 1.0000e-05\n",
      "Epoch 94/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 92.0859 - mae: 8.1856 - val_loss: 70.0896 - val_mae: 6.3003 - learning_rate: 1.0000e-05\n",
      "Epoch 95/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 92.0672 - mae: 8.1847 - val_loss: 70.0678 - val_mae: 6.2988 - learning_rate: 1.0000e-05\n",
      "Epoch 96/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 92.0488 - mae: 8.1838 - val_loss: 70.0461 - val_mae: 6.2973 - learning_rate: 1.0000e-05\n",
      "Epoch 97/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 92.0305 - mae: 8.1830 - val_loss: 70.0243 - val_mae: 6.2958 - learning_rate: 1.0000e-05\n",
      "Epoch 98/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 92.0127 - mae: 8.1821 - val_loss: 70.0024 - val_mae: 6.2942 - learning_rate: 1.0000e-05\n",
      "Epoch 99/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 91.9941 - mae: 8.1812 - val_loss: 69.9805 - val_mae: 6.2927 - learning_rate: 1.0000e-05\n",
      "Epoch 100/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 91.9767 - mae: 8.1804 - val_loss: 69.9586 - val_mae: 6.2912 - learning_rate: 1.0000e-05\n",
      "Epoch 101/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 91.9583 - mae: 8.1795 - val_loss: 69.9368 - val_mae: 6.2897 - learning_rate: 1.0000e-05\n",
      "Epoch 102/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 91.9412 - mae: 8.1786 - val_loss: 69.9149 - val_mae: 6.2882 - learning_rate: 1.0000e-05\n",
      "Epoch 103/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 91.9234 - mae: 8.1778 - val_loss: 69.8928 - val_mae: 6.2867 - learning_rate: 1.0000e-05\n",
      "Epoch 104/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 91.9055 - mae: 8.1769 - val_loss: 69.8710 - val_mae: 6.2852 - learning_rate: 1.0000e-05\n",
      "Epoch 105/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 91.8877 - mae: 8.1760 - val_loss: 69.8492 - val_mae: 6.2837 - learning_rate: 1.0000e-05\n",
      "Epoch 106/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 91.8698 - mae: 8.1752 - val_loss: 69.8273 - val_mae: 6.2822 - learning_rate: 1.0000e-05\n",
      "Epoch 107/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 91.8519 - mae: 8.1743 - val_loss: 69.8055 - val_mae: 6.2807 - learning_rate: 1.0000e-05\n",
      "Epoch 108/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 91.8351 - mae: 8.1734 - val_loss: 69.7836 - val_mae: 6.2791 - learning_rate: 1.0000e-05\n",
      "Epoch 109/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 91.8175 - mae: 8.1726 - val_loss: 69.7616 - val_mae: 6.2776 - learning_rate: 1.0000e-05\n",
      "Epoch 110/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 91.8001 - mae: 8.1716 - val_loss: 69.7397 - val_mae: 6.2761 - learning_rate: 1.0000e-05\n",
      "Epoch 111/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 91.7826 - mae: 8.1708 - val_loss: 69.7178 - val_mae: 6.2746 - learning_rate: 1.0000e-05\n",
      "Epoch 112/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 91.7652 - mae: 8.1699 - val_loss: 69.6959 - val_mae: 6.2731 - learning_rate: 1.0000e-05\n",
      "Epoch 113/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 91.7481 - mae: 8.1690 - val_loss: 69.6741 - val_mae: 6.2716 - learning_rate: 1.0000e-05\n",
      "Epoch 114/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 91.7297 - mae: 8.1681 - val_loss: 69.6524 - val_mae: 6.2702 - learning_rate: 1.0000e-05\n",
      "Epoch 115/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 91.7134 - mae: 8.1673 - val_loss: 69.6307 - val_mae: 6.2687 - learning_rate: 1.0000e-05\n",
      "Epoch 116/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 91.6954 - mae: 8.1664 - val_loss: 69.6090 - val_mae: 6.2672 - learning_rate: 1.0000e-05\n",
      "Epoch 117/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 91.6785 - mae: 8.1655 - val_loss: 69.5871 - val_mae: 6.2658 - learning_rate: 1.0000e-05\n",
      "Epoch 118/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 91.6608 - mae: 8.1646 - val_loss: 69.5653 - val_mae: 6.2643 - learning_rate: 1.0000e-05\n",
      "Epoch 119/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 91.6442 - mae: 8.1638 - val_loss: 69.5434 - val_mae: 6.2628 - learning_rate: 1.0000e-05\n",
      "Epoch 120/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 91.6265 - mae: 8.1629 - val_loss: 69.5216 - val_mae: 6.2613 - learning_rate: 1.0000e-05\n",
      "Epoch 121/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 91.6092 - mae: 8.1620 - val_loss: 69.4998 - val_mae: 6.2599 - learning_rate: 1.0000e-05\n",
      "Epoch 122/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 91.5923 - mae: 8.1611 - val_loss: 69.4779 - val_mae: 6.2584 - learning_rate: 1.0000e-05\n",
      "Epoch 123/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 91.5751 - mae: 8.1602 - val_loss: 69.4561 - val_mae: 6.2569 - learning_rate: 1.0000e-05\n",
      "Epoch 124/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 91.5576 - mae: 8.1593 - val_loss: 69.4342 - val_mae: 6.2554 - learning_rate: 1.0000e-05\n",
      "Epoch 125/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 91.5409 - mae: 8.1585 - val_loss: 69.4124 - val_mae: 6.2540 - learning_rate: 1.0000e-05\n",
      "Epoch 126/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 91.5235 - mae: 8.1576 - val_loss: 69.3905 - val_mae: 6.2525 - learning_rate: 1.0000e-05\n",
      "Epoch 127/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 91.5064 - mae: 8.1567 - val_loss: 69.3686 - val_mae: 6.2510 - learning_rate: 1.0000e-05\n",
      "Epoch 128/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 91.4895 - mae: 8.1558 - val_loss: 69.3467 - val_mae: 6.2495 - learning_rate: 1.0000e-05\n",
      "Epoch 129/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 91.4719 - mae: 8.1549 - val_loss: 69.3248 - val_mae: 6.2480 - learning_rate: 1.0000e-05\n",
      "Epoch 130/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 91.4550 - mae: 8.1540 - val_loss: 69.3029 - val_mae: 6.2465 - learning_rate: 1.0000e-05\n",
      "Epoch 131/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 91.4381 - mae: 8.1531 - val_loss: 69.2808 - val_mae: 6.2451 - learning_rate: 1.0000e-05\n",
      "Epoch 132/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 91.4211 - mae: 8.1523 - val_loss: 69.2587 - val_mae: 6.2436 - learning_rate: 1.0000e-05\n",
      "Epoch 133/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 91.4038 - mae: 8.1513 - val_loss: 69.2367 - val_mae: 6.2421 - learning_rate: 1.0000e-05\n",
      "Epoch 134/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 91.3873 - mae: 8.1505 - val_loss: 69.2146 - val_mae: 6.2406 - learning_rate: 1.0000e-05\n",
      "Epoch 135/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 91.3697 - mae: 8.1496 - val_loss: 69.1924 - val_mae: 6.2391 - learning_rate: 1.0000e-05\n",
      "Epoch 136/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 91.3528 - mae: 8.1487 - val_loss: 69.1704 - val_mae: 6.2376 - learning_rate: 1.0000e-05\n",
      "Epoch 137/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 91.3361 - mae: 8.1478 - val_loss: 69.1483 - val_mae: 6.2361 - learning_rate: 1.0000e-05\n",
      "Epoch 138/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 91.3187 - mae: 8.1469 - val_loss: 69.1261 - val_mae: 6.2346 - learning_rate: 1.0000e-05\n",
      "Epoch 139/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 91.3012 - mae: 8.1460 - val_loss: 69.1041 - val_mae: 6.2331 - learning_rate: 1.0000e-05\n",
      "Epoch 140/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 91.2848 - mae: 8.1451 - val_loss: 69.0818 - val_mae: 6.2316 - learning_rate: 1.0000e-05\n",
      "Epoch 141/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 91.2678 - mae: 8.1442 - val_loss: 69.0597 - val_mae: 6.2301 - learning_rate: 1.0000e-05\n",
      "Epoch 142/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 91.2507 - mae: 8.1433 - val_loss: 69.0375 - val_mae: 6.2286 - learning_rate: 1.0000e-05\n",
      "Epoch 143/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 91.2339 - mae: 8.1424 - val_loss: 69.0154 - val_mae: 6.2271 - learning_rate: 1.0000e-05\n",
      "Epoch 144/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 91.2170 - mae: 8.1415 - val_loss: 68.9933 - val_mae: 6.2256 - learning_rate: 1.0000e-05\n",
      "Epoch 145/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 91.2004 - mae: 8.1406 - val_loss: 68.9710 - val_mae: 6.2241 - learning_rate: 1.0000e-05\n",
      "Epoch 146/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 91.1828 - mae: 8.1397 - val_loss: 68.9489 - val_mae: 6.2226 - learning_rate: 1.0000e-05\n",
      "Epoch 147/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 91.1662 - mae: 8.1388 - val_loss: 68.9267 - val_mae: 6.2211 - learning_rate: 1.0000e-05\n",
      "Epoch 148/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 91.1493 - mae: 8.1380 - val_loss: 68.9045 - val_mae: 6.2196 - learning_rate: 1.0000e-05\n",
      "Epoch 149/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 91.1327 - mae: 8.1371 - val_loss: 68.8822 - val_mae: 6.2181 - learning_rate: 1.0000e-05\n",
      "Epoch 150/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 91.1160 - mae: 8.1362 - val_loss: 68.8600 - val_mae: 6.2165 - learning_rate: 1.0000e-05\n",
      "Epoch 151/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 91.0992 - mae: 8.1353 - val_loss: 68.8377 - val_mae: 6.2150 - learning_rate: 1.0000e-05\n",
      "Epoch 152/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 91.0819 - mae: 8.1345 - val_loss: 68.8153 - val_mae: 6.2135 - learning_rate: 1.0000e-05\n",
      "Epoch 153/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 91.0655 - mae: 8.1336 - val_loss: 68.7930 - val_mae: 6.2120 - learning_rate: 1.0000e-05\n",
      "Epoch 154/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 91.0487 - mae: 8.1327 - val_loss: 68.7706 - val_mae: 6.2105 - learning_rate: 1.0000e-05\n",
      "Epoch 155/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 91.0317 - mae: 8.1318 - val_loss: 68.7482 - val_mae: 6.2090 - learning_rate: 1.0000e-05\n",
      "Epoch 156/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 91.0150 - mae: 8.1310 - val_loss: 68.7258 - val_mae: 6.2075 - learning_rate: 1.0000e-05\n",
      "Epoch 157/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.9983 - mae: 8.1301 - val_loss: 68.7033 - val_mae: 6.2060 - learning_rate: 1.0000e-05\n",
      "Epoch 158/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.9816 - mae: 8.1293 - val_loss: 68.6808 - val_mae: 6.2044 - learning_rate: 1.0000e-05\n",
      "Epoch 159/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.9648 - mae: 8.1284 - val_loss: 68.6583 - val_mae: 6.2029 - learning_rate: 1.0000e-05\n",
      "Epoch 160/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.9479 - mae: 8.1275 - val_loss: 68.6359 - val_mae: 6.2014 - learning_rate: 1.0000e-05\n",
      "Epoch 161/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.9311 - mae: 8.1266 - val_loss: 68.6135 - val_mae: 6.1999 - learning_rate: 1.0000e-05\n",
      "Epoch 162/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 90.9143 - mae: 8.1257 - val_loss: 68.5910 - val_mae: 6.1984 - learning_rate: 1.0000e-05\n",
      "Epoch 163/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.8976 - mae: 8.1249 - val_loss: 68.5686 - val_mae: 6.1968 - learning_rate: 1.0000e-05\n",
      "Epoch 164/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 90.8810 - mae: 8.1240 - val_loss: 68.5460 - val_mae: 6.1953 - learning_rate: 1.0000e-05\n",
      "Epoch 165/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.8637 - mae: 8.1231 - val_loss: 68.5235 - val_mae: 6.1938 - learning_rate: 1.0000e-05\n",
      "Epoch 166/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 90.8466 - mae: 8.1222 - val_loss: 68.5010 - val_mae: 6.1923 - learning_rate: 1.0000e-05\n",
      "Epoch 167/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 90.8304 - mae: 8.1214 - val_loss: 68.4785 - val_mae: 6.1908 - learning_rate: 1.0000e-05\n",
      "Epoch 168/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.8137 - mae: 8.1205 - val_loss: 68.4560 - val_mae: 6.1892 - learning_rate: 1.0000e-05\n",
      "Epoch 169/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.7966 - mae: 8.1196 - val_loss: 68.4334 - val_mae: 6.1877 - learning_rate: 1.0000e-05\n",
      "Epoch 170/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.7799 - mae: 8.1187 - val_loss: 68.4109 - val_mae: 6.1862 - learning_rate: 1.0000e-05\n",
      "Epoch 171/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.7632 - mae: 8.1179 - val_loss: 68.3883 - val_mae: 6.1847 - learning_rate: 1.0000e-05\n",
      "Epoch 172/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.7466 - mae: 8.1170 - val_loss: 68.3657 - val_mae: 6.1831 - learning_rate: 1.0000e-05\n",
      "Epoch 173/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 90.7297 - mae: 8.1161 - val_loss: 68.3431 - val_mae: 6.1816 - learning_rate: 1.0000e-05\n",
      "Epoch 174/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 90.7126 - mae: 8.1153 - val_loss: 68.3205 - val_mae: 6.1801 - learning_rate: 1.0000e-05\n",
      "Epoch 175/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.6957 - mae: 8.1144 - val_loss: 68.2980 - val_mae: 6.1786 - learning_rate: 1.0000e-05\n",
      "Epoch 176/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.6791 - mae: 8.1135 - val_loss: 68.2754 - val_mae: 6.1770 - learning_rate: 1.0000e-05\n",
      "Epoch 177/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 90.6621 - mae: 8.1127 - val_loss: 68.2529 - val_mae: 6.1755 - learning_rate: 1.0000e-05\n",
      "Epoch 178/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 90.6453 - mae: 8.1118 - val_loss: 68.2304 - val_mae: 6.1740 - learning_rate: 1.0000e-05\n",
      "Epoch 179/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 90.6287 - mae: 8.1109 - val_loss: 68.2078 - val_mae: 6.1725 - learning_rate: 1.0000e-05\n",
      "Epoch 180/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 90.6117 - mae: 8.1100 - val_loss: 68.1852 - val_mae: 6.1709 - learning_rate: 1.0000e-05\n",
      "Epoch 181/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.5943 - mae: 8.1091 - val_loss: 68.1627 - val_mae: 6.1694 - learning_rate: 1.0000e-05\n",
      "Epoch 182/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.5779 - mae: 8.1083 - val_loss: 68.1400 - val_mae: 6.1679 - learning_rate: 1.0000e-05\n",
      "Epoch 183/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 90.5609 - mae: 8.1074 - val_loss: 68.1172 - val_mae: 6.1663 - learning_rate: 1.0000e-05\n",
      "Epoch 184/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 90.5442 - mae: 8.1065 - val_loss: 68.0944 - val_mae: 6.1648 - learning_rate: 1.0000e-05\n",
      "Epoch 185/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.5269 - mae: 8.1056 - val_loss: 68.0717 - val_mae: 6.1633 - learning_rate: 1.0000e-05\n",
      "Epoch 186/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 90.5102 - mae: 8.1048 - val_loss: 68.0490 - val_mae: 6.1617 - learning_rate: 1.0000e-05\n",
      "Epoch 187/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.4931 - mae: 8.1039 - val_loss: 68.0263 - val_mae: 6.1602 - learning_rate: 1.0000e-05\n",
      "Epoch 188/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.4763 - mae: 8.1030 - val_loss: 68.0035 - val_mae: 6.1586 - learning_rate: 1.0000e-05\n",
      "Epoch 189/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 90.4589 - mae: 8.1021 - val_loss: 67.9809 - val_mae: 6.1571 - learning_rate: 1.0000e-05\n",
      "Epoch 190/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.4423 - mae: 8.1012 - val_loss: 67.9582 - val_mae: 6.1556 - learning_rate: 1.0000e-05\n",
      "Epoch 191/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.4252 - mae: 8.1003 - val_loss: 67.9355 - val_mae: 6.1540 - learning_rate: 1.0000e-05\n",
      "Epoch 192/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.4079 - mae: 8.0994 - val_loss: 67.9128 - val_mae: 6.1525 - learning_rate: 1.0000e-05\n",
      "Epoch 193/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.3907 - mae: 8.0985 - val_loss: 67.8902 - val_mae: 6.1509 - learning_rate: 1.0000e-05\n",
      "Epoch 194/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.3739 - mae: 8.0976 - val_loss: 67.8675 - val_mae: 6.1494 - learning_rate: 1.0000e-05\n",
      "Epoch 195/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 90.3563 - mae: 8.0967 - val_loss: 67.8448 - val_mae: 6.1478 - learning_rate: 1.0000e-05\n",
      "Epoch 196/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 90.3392 - mae: 8.0959 - val_loss: 67.8220 - val_mae: 6.1463 - learning_rate: 1.0000e-05\n",
      "Epoch 197/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.3226 - mae: 8.0950 - val_loss: 67.7991 - val_mae: 6.1447 - learning_rate: 1.0000e-05\n",
      "Epoch 198/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.3049 - mae: 8.0941 - val_loss: 67.7763 - val_mae: 6.1432 - learning_rate: 1.0000e-05\n",
      "Epoch 199/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.2880 - mae: 8.0932 - val_loss: 67.7534 - val_mae: 6.1416 - learning_rate: 1.0000e-05\n",
      "Epoch 200/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 90.2701 - mae: 8.0923 - val_loss: 67.7306 - val_mae: 6.1401 - learning_rate: 1.0000e-05\n",
      "Epoch 201/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 90.2531 - mae: 8.0913 - val_loss: 67.7077 - val_mae: 6.1385 - learning_rate: 1.0000e-05\n",
      "Epoch 202/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.2359 - mae: 8.0904 - val_loss: 67.6849 - val_mae: 6.1370 - learning_rate: 1.0000e-05\n",
      "Epoch 203/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 90.2188 - mae: 8.0895 - val_loss: 67.6619 - val_mae: 6.1354 - learning_rate: 1.0000e-05\n",
      "Epoch 204/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.2012 - mae: 8.0886 - val_loss: 67.6390 - val_mae: 6.1338 - learning_rate: 1.0000e-05\n",
      "Epoch 205/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.1837 - mae: 8.0877 - val_loss: 67.6160 - val_mae: 6.1323 - learning_rate: 1.0000e-05\n",
      "Epoch 206/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 90.1659 - mae: 8.0868 - val_loss: 67.5930 - val_mae: 6.1307 - learning_rate: 1.0000e-05\n",
      "Epoch 207/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.1489 - mae: 8.0859 - val_loss: 67.5700 - val_mae: 6.1291 - learning_rate: 1.0000e-05\n",
      "Epoch 208/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 90.1315 - mae: 8.0850 - val_loss: 67.5470 - val_mae: 6.1276 - learning_rate: 1.0000e-05\n",
      "Epoch 209/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 90.1140 - mae: 8.0841 - val_loss: 67.5241 - val_mae: 6.1260 - learning_rate: 1.0000e-05\n",
      "Epoch 210/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 90.0958 - mae: 8.0832 - val_loss: 67.5016 - val_mae: 6.1246 - learning_rate: 1.0000e-05\n",
      "Epoch 211/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 90.0785 - mae: 8.0823 - val_loss: 67.4790 - val_mae: 6.1231 - learning_rate: 1.0000e-05\n",
      "Epoch 212/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 90.0615 - mae: 8.0814 - val_loss: 67.4564 - val_mae: 6.1216 - learning_rate: 1.0000e-05\n",
      "Epoch 213/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 90.0434 - mae: 8.0804 - val_loss: 67.4339 - val_mae: 6.1201 - learning_rate: 1.0000e-05\n",
      "Epoch 214/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 90.0258 - mae: 8.0795 - val_loss: 67.4114 - val_mae: 6.1186 - learning_rate: 1.0000e-05\n",
      "Epoch 215/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 90.0086 - mae: 8.0786 - val_loss: 67.3887 - val_mae: 6.1171 - learning_rate: 1.0000e-05\n",
      "Epoch 216/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 89.9909 - mae: 8.0777 - val_loss: 67.3662 - val_mae: 6.1157 - learning_rate: 1.0000e-05\n",
      "Epoch 217/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 89.9727 - mae: 8.0768 - val_loss: 67.3437 - val_mae: 6.1142 - learning_rate: 1.0000e-05\n",
      "Epoch 218/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 89.9554 - mae: 8.0759 - val_loss: 67.3211 - val_mae: 6.1127 - learning_rate: 1.0000e-05\n",
      "Epoch 219/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 89.9377 - mae: 8.0750 - val_loss: 67.2986 - val_mae: 6.1112 - learning_rate: 1.0000e-05\n",
      "Epoch 220/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 89.9200 - mae: 8.0740 - val_loss: 67.2760 - val_mae: 6.1097 - learning_rate: 1.0000e-05\n",
      "Epoch 221/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 89.9026 - mae: 8.0732 - val_loss: 67.2533 - val_mae: 6.1082 - learning_rate: 1.0000e-05\n",
      "Epoch 222/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 89.8844 - mae: 8.0722 - val_loss: 67.2307 - val_mae: 6.1068 - learning_rate: 1.0000e-05\n",
      "Epoch 223/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 89.8666 - mae: 8.0713 - val_loss: 67.2080 - val_mae: 6.1053 - learning_rate: 1.0000e-05\n",
      "Epoch 224/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 89.8480 - mae: 8.0704 - val_loss: 67.1854 - val_mae: 6.1038 - learning_rate: 1.0000e-05\n",
      "Epoch 225/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 89.8304 - mae: 8.0695 - val_loss: 67.1628 - val_mae: 6.1023 - learning_rate: 1.0000e-05\n",
      "Epoch 226/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 89.8129 - mae: 8.0685 - val_loss: 67.1401 - val_mae: 6.1008 - learning_rate: 1.0000e-05\n",
      "Epoch 227/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 89.7949 - mae: 8.0676 - val_loss: 67.1174 - val_mae: 6.0993 - learning_rate: 1.0000e-05\n",
      "Epoch 228/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 89.7769 - mae: 8.0667 - val_loss: 67.0947 - val_mae: 6.0978 - learning_rate: 1.0000e-05\n",
      "Epoch 229/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 89.7589 - mae: 8.0658 - val_loss: 67.0726 - val_mae: 6.0964 - learning_rate: 1.0000e-05\n",
      "Epoch 230/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 89.7411 - mae: 8.0649 - val_loss: 67.0508 - val_mae: 6.0950 - learning_rate: 1.0000e-05\n",
      "Epoch 231/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 89.7229 - mae: 8.0639 - val_loss: 67.0292 - val_mae: 6.0935 - learning_rate: 1.0000e-05\n",
      "Epoch 232/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 89.7042 - mae: 8.0630 - val_loss: 67.0078 - val_mae: 6.0922 - learning_rate: 1.0000e-05\n",
      "Epoch 233/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 89.6868 - mae: 8.0621 - val_loss: 66.9864 - val_mae: 6.0909 - learning_rate: 1.0000e-05\n",
      "Epoch 234/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 89.6685 - mae: 8.0612 - val_loss: 66.9649 - val_mae: 6.0895 - learning_rate: 1.0000e-05\n",
      "Epoch 235/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 89.6504 - mae: 8.0602 - val_loss: 66.9435 - val_mae: 6.0882 - learning_rate: 1.0000e-05\n",
      "Epoch 236/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 89.6324 - mae: 8.0593 - val_loss: 66.9221 - val_mae: 6.0868 - learning_rate: 1.0000e-05\n",
      "Epoch 237/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 89.6143 - mae: 8.0584 - val_loss: 66.9007 - val_mae: 6.0855 - learning_rate: 1.0000e-05\n",
      "Epoch 238/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 89.5961 - mae: 8.0574 - val_loss: 66.8794 - val_mae: 6.0841 - learning_rate: 1.0000e-05\n",
      "Epoch 239/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 89.5778 - mae: 8.0565 - val_loss: 66.8581 - val_mae: 6.0828 - learning_rate: 1.0000e-05\n",
      "Epoch 240/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 89.5591 - mae: 8.0556 - val_loss: 66.8368 - val_mae: 6.0815 - learning_rate: 1.0000e-05\n",
      "Epoch 241/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 89.5416 - mae: 8.0547 - val_loss: 66.8154 - val_mae: 6.0801 - learning_rate: 1.0000e-05\n",
      "Epoch 242/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 89.5233 - mae: 8.0537 - val_loss: 66.7940 - val_mae: 6.0788 - learning_rate: 1.0000e-05\n",
      "Epoch 243/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 89.5043 - mae: 8.0528 - val_loss: 66.7727 - val_mae: 6.0774 - learning_rate: 1.0000e-05\n",
      "Epoch 244/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 89.4865 - mae: 8.0519 - val_loss: 66.7513 - val_mae: 6.0761 - learning_rate: 1.0000e-05\n",
      "Epoch 245/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 89.4689 - mae: 8.0510 - val_loss: 66.7299 - val_mae: 6.0747 - learning_rate: 1.0000e-05\n",
      "Epoch 246/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 89.4508 - mae: 8.0501 - val_loss: 66.7086 - val_mae: 6.0734 - learning_rate: 1.0000e-05\n",
      "Epoch 247/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 89.4333 - mae: 8.0492 - val_loss: 66.6873 - val_mae: 6.0720 - learning_rate: 1.0000e-05\n",
      "Epoch 248/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 89.4146 - mae: 8.0483 - val_loss: 66.6659 - val_mae: 6.0707 - learning_rate: 1.0000e-05\n",
      "Epoch 249/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 89.3973 - mae: 8.0474 - val_loss: 66.6445 - val_mae: 6.0693 - learning_rate: 1.0000e-05\n",
      "Epoch 250/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 89.3791 - mae: 8.0465 - val_loss: 66.6232 - val_mae: 6.0680 - learning_rate: 1.0000e-05\n",
      "Epoch 251/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 89.3619 - mae: 8.0456 - val_loss: 66.6016 - val_mae: 6.0666 - learning_rate: 1.0000e-05\n",
      "Epoch 252/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 89.3441 - mae: 8.0447 - val_loss: 66.5802 - val_mae: 6.0653 - learning_rate: 1.0000e-05\n",
      "Epoch 253/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 89.3256 - mae: 8.0438 - val_loss: 66.5588 - val_mae: 6.0639 - learning_rate: 1.0000e-05\n",
      "Epoch 254/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 89.3077 - mae: 8.0429 - val_loss: 66.5374 - val_mae: 6.0626 - learning_rate: 1.0000e-05\n",
      "Epoch 255/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 89.2899 - mae: 8.0420 - val_loss: 66.5160 - val_mae: 6.0612 - learning_rate: 1.0000e-05\n",
      "Epoch 256/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 89.2720 - mae: 8.0411 - val_loss: 66.4946 - val_mae: 6.0598 - learning_rate: 1.0000e-05\n",
      "Epoch 257/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 89.2541 - mae: 8.0402 - val_loss: 66.4731 - val_mae: 6.0585 - learning_rate: 1.0000e-05\n",
      "Epoch 258/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 89.2362 - mae: 8.0393 - val_loss: 66.4517 - val_mae: 6.0571 - learning_rate: 1.0000e-05\n",
      "Epoch 259/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 89.2185 - mae: 8.0384 - val_loss: 66.4302 - val_mae: 6.0557 - learning_rate: 1.0000e-05\n",
      "Epoch 260/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 89.2002 - mae: 8.0375 - val_loss: 66.4087 - val_mae: 6.0544 - learning_rate: 1.0000e-05\n",
      "Epoch 261/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 89.1826 - mae: 8.0366 - val_loss: 66.3872 - val_mae: 6.0530 - learning_rate: 1.0000e-05\n",
      "Epoch 262/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 89.1644 - mae: 8.0357 - val_loss: 66.3656 - val_mae: 6.0517 - learning_rate: 1.0000e-05\n",
      "Epoch 263/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 89.1465 - mae: 8.0348 - val_loss: 66.3441 - val_mae: 6.0503 - learning_rate: 1.0000e-05\n",
      "Epoch 264/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 89.1289 - mae: 8.0339 - val_loss: 66.3226 - val_mae: 6.0489 - learning_rate: 1.0000e-05\n",
      "Epoch 265/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 89.1110 - mae: 8.0330 - val_loss: 66.3009 - val_mae: 6.0475 - learning_rate: 1.0000e-05\n",
      "Epoch 266/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 89.0926 - mae: 8.0321 - val_loss: 66.2795 - val_mae: 6.0462 - learning_rate: 1.0000e-05\n",
      "Epoch 267/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 89.0751 - mae: 8.0312 - val_loss: 66.2579 - val_mae: 6.0448 - learning_rate: 1.0000e-05\n",
      "Epoch 268/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 89.0567 - mae: 8.0303 - val_loss: 66.2364 - val_mae: 6.0434 - learning_rate: 1.0000e-05\n",
      "Epoch 269/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 89.0393 - mae: 8.0294 - val_loss: 66.2148 - val_mae: 6.0420 - learning_rate: 1.0000e-05\n",
      "Epoch 270/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 89.0217 - mae: 8.0285 - val_loss: 66.1932 - val_mae: 6.0407 - learning_rate: 1.0000e-05\n",
      "Epoch 271/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 89.0032 - mae: 8.0276 - val_loss: 66.1716 - val_mae: 6.0393 - learning_rate: 1.0000e-05\n",
      "Epoch 272/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 88.9855 - mae: 8.0267 - val_loss: 66.1500 - val_mae: 6.0379 - learning_rate: 1.0000e-05\n",
      "Epoch 273/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 88.9672 - mae: 8.0258 - val_loss: 66.1285 - val_mae: 6.0366 - learning_rate: 1.0000e-05\n",
      "Epoch 274/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 88.9496 - mae: 8.0249 - val_loss: 66.1069 - val_mae: 6.0352 - learning_rate: 1.0000e-05\n",
      "Epoch 275/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 88.9312 - mae: 8.0240 - val_loss: 66.0853 - val_mae: 6.0338 - learning_rate: 1.0000e-05\n",
      "Epoch 276/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 88.9138 - mae: 8.0231 - val_loss: 66.0637 - val_mae: 6.0324 - learning_rate: 1.0000e-05\n",
      "Epoch 277/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 88.8956 - mae: 8.0222 - val_loss: 66.0421 - val_mae: 6.0311 - learning_rate: 1.0000e-05\n",
      "Epoch 278/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 88.8770 - mae: 8.0212 - val_loss: 66.0206 - val_mae: 6.0297 - learning_rate: 1.0000e-05\n",
      "Epoch 279/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 88.8596 - mae: 8.0203 - val_loss: 65.9989 - val_mae: 6.0283 - learning_rate: 1.0000e-05\n",
      "Epoch 280/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 88.8412 - mae: 8.0194 - val_loss: 65.9773 - val_mae: 6.0269 - learning_rate: 1.0000e-05\n",
      "Epoch 281/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 88.8234 - mae: 8.0185 - val_loss: 65.9557 - val_mae: 6.0255 - learning_rate: 1.0000e-05\n",
      "Epoch 282/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 88.8058 - mae: 8.0176 - val_loss: 65.9339 - val_mae: 6.0242 - learning_rate: 1.0000e-05\n",
      "Epoch 283/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 88.7869 - mae: 8.0166 - val_loss: 65.9124 - val_mae: 6.0228 - learning_rate: 1.0000e-05\n",
      "Epoch 284/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 88.7696 - mae: 8.0158 - val_loss: 65.8906 - val_mae: 6.0214 - learning_rate: 1.0000e-05\n",
      "Epoch 285/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 88.7510 - mae: 8.0148 - val_loss: 65.8691 - val_mae: 6.0200 - learning_rate: 1.0000e-05\n",
      "Epoch 286/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 88.7333 - mae: 8.0139 - val_loss: 65.8474 - val_mae: 6.0186 - learning_rate: 1.0000e-05\n",
      "Epoch 287/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 88.7148 - mae: 8.0130 - val_loss: 65.8257 - val_mae: 6.0172 - learning_rate: 1.0000e-05\n",
      "Epoch 288/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 88.6970 - mae: 8.0121 - val_loss: 65.8041 - val_mae: 6.0159 - learning_rate: 1.0000e-05\n",
      "Epoch 289/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 88.6785 - mae: 8.0111 - val_loss: 65.7824 - val_mae: 6.0145 - learning_rate: 1.0000e-05\n",
      "Epoch 290/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 88.6607 - mae: 8.0102 - val_loss: 65.7607 - val_mae: 6.0131 - learning_rate: 1.0000e-05\n",
      "Epoch 291/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 88.6430 - mae: 8.0093 - val_loss: 65.7390 - val_mae: 6.0117 - learning_rate: 1.0000e-05\n",
      "Epoch 292/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 88.6248 - mae: 8.0084 - val_loss: 65.7174 - val_mae: 6.0103 - learning_rate: 1.0000e-05\n",
      "Epoch 293/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 88.6058 - mae: 8.0074 - val_loss: 65.6958 - val_mae: 6.0090 - learning_rate: 1.0000e-05\n",
      "Epoch 294/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 88.5879 - mae: 8.0065 - val_loss: 65.6741 - val_mae: 6.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 295/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 88.5697 - mae: 8.0056 - val_loss: 65.6526 - val_mae: 6.0062 - learning_rate: 1.0000e-05\n",
      "Epoch 296/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 88.5516 - mae: 8.0047 - val_loss: 65.6309 - val_mae: 6.0048 - learning_rate: 1.0000e-05\n",
      "Epoch 297/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 88.5340 - mae: 8.0037 - val_loss: 65.6091 - val_mae: 6.0034 - learning_rate: 1.0000e-05\n",
      "Epoch 298/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 88.5150 - mae: 8.0028 - val_loss: 65.5874 - val_mae: 6.0020 - learning_rate: 1.0000e-05\n",
      "Epoch 299/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 88.4968 - mae: 8.0019 - val_loss: 65.5656 - val_mae: 6.0006 - learning_rate: 1.0000e-05\n",
      "Epoch 300/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 88.4788 - mae: 8.0009 - val_loss: 65.5439 - val_mae: 5.9992 - learning_rate: 1.0000e-05\n",
      "Epoch 301/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 88.4609 - mae: 8.0000 - val_loss: 65.5221 - val_mae: 5.9979 - learning_rate: 1.0000e-05\n",
      "Epoch 302/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 88.4421 - mae: 7.9991 - val_loss: 65.5003 - val_mae: 5.9965 - learning_rate: 1.0000e-05\n",
      "Epoch 303/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 88.4242 - mae: 7.9981 - val_loss: 65.4786 - val_mae: 5.9951 - learning_rate: 1.0000e-05\n",
      "Epoch 304/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 88.4055 - mae: 7.9972 - val_loss: 65.4569 - val_mae: 5.9937 - learning_rate: 1.0000e-05\n",
      "Epoch 305/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 88.3873 - mae: 7.9963 - val_loss: 65.4352 - val_mae: 5.9923 - learning_rate: 1.0000e-05\n",
      "Epoch 306/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 88.3689 - mae: 7.9953 - val_loss: 65.4135 - val_mae: 5.9909 - learning_rate: 1.0000e-05\n",
      "Epoch 307/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 88.3513 - mae: 7.9944 - val_loss: 65.3919 - val_mae: 5.9895 - learning_rate: 1.0000e-05\n",
      "Epoch 308/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 88.3322 - mae: 7.9934 - val_loss: 65.3703 - val_mae: 5.9881 - learning_rate: 1.0000e-05\n",
      "Epoch 309/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 88.3144 - mae: 7.9925 - val_loss: 65.3486 - val_mae: 5.9867 - learning_rate: 1.0000e-05\n",
      "Epoch 310/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 88.2961 - mae: 7.9915 - val_loss: 65.3270 - val_mae: 5.9853 - learning_rate: 1.0000e-05\n",
      "Epoch 311/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 88.2776 - mae: 7.9906 - val_loss: 65.3054 - val_mae: 5.9839 - learning_rate: 1.0000e-05\n",
      "Epoch 312/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 88.2590 - mae: 7.9896 - val_loss: 65.2838 - val_mae: 5.9826 - learning_rate: 1.0000e-05\n",
      "Epoch 313/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 88.2408 - mae: 7.9887 - val_loss: 65.2621 - val_mae: 5.9812 - learning_rate: 1.0000e-05\n",
      "Epoch 314/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 88.2227 - mae: 7.9878 - val_loss: 65.2404 - val_mae: 5.9798 - learning_rate: 1.0000e-05\n",
      "Epoch 315/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 88.2042 - mae: 7.9868 - val_loss: 65.2188 - val_mae: 5.9784 - learning_rate: 1.0000e-05\n",
      "Epoch 316/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 88.1863 - mae: 7.9859 - val_loss: 65.1971 - val_mae: 5.9770 - learning_rate: 1.0000e-05\n",
      "Epoch 317/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 88.1677 - mae: 7.9849 - val_loss: 65.1755 - val_mae: 5.9756 - learning_rate: 1.0000e-05\n",
      "Epoch 318/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 88.1494 - mae: 7.9840 - val_loss: 65.1539 - val_mae: 5.9742 - learning_rate: 1.0000e-05\n",
      "Epoch 319/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 88.1304 - mae: 7.9830 - val_loss: 65.1324 - val_mae: 5.9728 - learning_rate: 1.0000e-05\n",
      "Epoch 320/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 88.1124 - mae: 7.9820 - val_loss: 65.1108 - val_mae: 5.9714 - learning_rate: 1.0000e-05\n",
      "Epoch 321/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 88.0942 - mae: 7.9811 - val_loss: 65.0892 - val_mae: 5.9700 - learning_rate: 1.0000e-05\n",
      "Epoch 322/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 88.0760 - mae: 7.9801 - val_loss: 65.0677 - val_mae: 5.9686 - learning_rate: 1.0000e-05\n",
      "Epoch 323/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 88.0573 - mae: 7.9792 - val_loss: 65.0462 - val_mae: 5.9672 - learning_rate: 1.0000e-05\n",
      "Epoch 324/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 88.0402 - mae: 7.9783 - val_loss: 65.0246 - val_mae: 5.9658 - learning_rate: 1.0000e-05\n",
      "Epoch 325/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 88.0219 - mae: 7.9774 - val_loss: 65.0030 - val_mae: 5.9644 - learning_rate: 1.0000e-05\n",
      "Epoch 326/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 88.0038 - mae: 7.9765 - val_loss: 64.9815 - val_mae: 5.9630 - learning_rate: 1.0000e-05\n",
      "Epoch 327/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 87.9857 - mae: 7.9755 - val_loss: 64.9599 - val_mae: 5.9616 - learning_rate: 1.0000e-05\n",
      "Epoch 328/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 87.9679 - mae: 7.9746 - val_loss: 64.9383 - val_mae: 5.9603 - learning_rate: 1.0000e-05\n",
      "Epoch 329/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 87.9501 - mae: 7.9737 - val_loss: 64.9166 - val_mae: 5.9589 - learning_rate: 1.0000e-05\n",
      "Epoch 330/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 87.9320 - mae: 7.9728 - val_loss: 64.8951 - val_mae: 5.9575 - learning_rate: 1.0000e-05\n",
      "Epoch 331/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 87.9142 - mae: 7.9719 - val_loss: 64.8736 - val_mae: 5.9561 - learning_rate: 1.0000e-05\n",
      "Epoch 332/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 87.8959 - mae: 7.9709 - val_loss: 64.8521 - val_mae: 5.9547 - learning_rate: 1.0000e-05\n",
      "Epoch 333/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 87.8781 - mae: 7.9700 - val_loss: 64.8306 - val_mae: 5.9533 - learning_rate: 1.0000e-05\n",
      "Epoch 334/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - loss: 87.8603 - mae: 7.9691 - val_loss: 64.8090 - val_mae: 5.9519 - learning_rate: 1.0000e-05\n",
      "Epoch 335/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 87.8427 - mae: 7.9681 - val_loss: 64.7874 - val_mae: 5.9505 - learning_rate: 1.0000e-05\n",
      "Epoch 336/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 87.8242 - mae: 7.9672 - val_loss: 64.7659 - val_mae: 5.9491 - learning_rate: 1.0000e-05\n",
      "Epoch 337/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 87.8064 - mae: 7.9663 - val_loss: 64.7443 - val_mae: 5.9477 - learning_rate: 1.0000e-05\n",
      "Epoch 338/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 87.7883 - mae: 7.9654 - val_loss: 64.7229 - val_mae: 5.9463 - learning_rate: 1.0000e-05\n",
      "Epoch 339/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 87.7704 - mae: 7.9644 - val_loss: 64.7014 - val_mae: 5.9449 - learning_rate: 1.0000e-05\n",
      "Epoch 340/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 87.7524 - mae: 7.9635 - val_loss: 64.6798 - val_mae: 5.9435 - learning_rate: 1.0000e-05\n",
      "Epoch 341/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 87.7344 - mae: 7.9626 - val_loss: 64.6583 - val_mae: 5.9421 - learning_rate: 1.0000e-05\n",
      "Epoch 342/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 87.7166 - mae: 7.9616 - val_loss: 64.6368 - val_mae: 5.9407 - learning_rate: 1.0000e-05\n",
      "Epoch 343/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 87.6982 - mae: 7.9607 - val_loss: 64.6153 - val_mae: 5.9393 - learning_rate: 1.0000e-05\n",
      "Epoch 344/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 87.6805 - mae: 7.9597 - val_loss: 64.5938 - val_mae: 5.9379 - learning_rate: 1.0000e-05\n",
      "Epoch 345/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 87.6622 - mae: 7.9588 - val_loss: 64.5723 - val_mae: 5.9365 - learning_rate: 1.0000e-05\n",
      "Epoch 346/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 87.6443 - mae: 7.9579 - val_loss: 64.5508 - val_mae: 5.9351 - learning_rate: 1.0000e-05\n",
      "Epoch 347/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 87.6264 - mae: 7.9569 - val_loss: 64.5293 - val_mae: 5.9337 - learning_rate: 1.0000e-05\n",
      "Epoch 348/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 87.6078 - mae: 7.9560 - val_loss: 64.5078 - val_mae: 5.9323 - learning_rate: 1.0000e-05\n",
      "Epoch 349/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - loss: 87.5898 - mae: 7.9550 - val_loss: 64.4863 - val_mae: 5.9309 - learning_rate: 1.0000e-05\n",
      "Epoch 350/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 87.5720 - mae: 7.9541 - val_loss: 64.4648 - val_mae: 5.9294 - learning_rate: 1.0000e-05\n",
      "Epoch 351/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 87.5538 - mae: 7.9531 - val_loss: 64.4433 - val_mae: 5.9280 - learning_rate: 1.0000e-05\n",
      "Epoch 352/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 87.5359 - mae: 7.9522 - val_loss: 64.4218 - val_mae: 5.9266 - learning_rate: 1.0000e-05\n",
      "Epoch 353/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 87.5176 - mae: 7.9512 - val_loss: 64.4003 - val_mae: 5.9252 - learning_rate: 1.0000e-05\n",
      "Epoch 354/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 87.4991 - mae: 7.9503 - val_loss: 64.3790 - val_mae: 5.9238 - learning_rate: 1.0000e-05\n",
      "Epoch 355/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 87.4815 - mae: 7.9493 - val_loss: 64.3576 - val_mae: 5.9224 - learning_rate: 1.0000e-05\n",
      "Epoch 356/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 87.4632 - mae: 7.9484 - val_loss: 64.3362 - val_mae: 5.9210 - learning_rate: 1.0000e-05\n",
      "Epoch 357/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 87.4451 - mae: 7.9474 - val_loss: 64.3148 - val_mae: 5.9196 - learning_rate: 1.0000e-05\n",
      "Epoch 358/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 87.4270 - mae: 7.9465 - val_loss: 64.2935 - val_mae: 5.9182 - learning_rate: 1.0000e-05\n",
      "Epoch 359/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 87.4087 - mae: 7.9455 - val_loss: 64.2721 - val_mae: 5.9168 - learning_rate: 1.0000e-05\n",
      "Epoch 360/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 87.3905 - mae: 7.9446 - val_loss: 64.2508 - val_mae: 5.9154 - learning_rate: 1.0000e-05\n",
      "Epoch 361/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 87.3727 - mae: 7.9436 - val_loss: 64.2294 - val_mae: 5.9140 - learning_rate: 1.0000e-05\n",
      "Epoch 362/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 87.3546 - mae: 7.9427 - val_loss: 64.2081 - val_mae: 5.9126 - learning_rate: 1.0000e-05\n",
      "Epoch 363/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 87.3363 - mae: 7.9417 - val_loss: 64.1869 - val_mae: 5.9112 - learning_rate: 1.0000e-05\n",
      "Epoch 364/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 87.3180 - mae: 7.9408 - val_loss: 64.1657 - val_mae: 5.9098 - learning_rate: 1.0000e-05\n",
      "Epoch 365/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 87.2998 - mae: 7.9398 - val_loss: 64.1445 - val_mae: 5.9085 - learning_rate: 1.0000e-05\n",
      "Epoch 366/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 87.2817 - mae: 7.9389 - val_loss: 64.1234 - val_mae: 5.9071 - learning_rate: 1.0000e-05\n",
      "Epoch 367/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 87.2639 - mae: 7.9379 - val_loss: 64.1021 - val_mae: 5.9057 - learning_rate: 1.0000e-05\n",
      "Epoch 368/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 87.2454 - mae: 7.9369 - val_loss: 64.0810 - val_mae: 5.9043 - learning_rate: 1.0000e-05\n",
      "Epoch 369/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 87.2269 - mae: 7.9360 - val_loss: 64.0599 - val_mae: 5.9029 - learning_rate: 1.0000e-05\n",
      "Epoch 370/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 87.2091 - mae: 7.9350 - val_loss: 64.0387 - val_mae: 5.9015 - learning_rate: 1.0000e-05\n",
      "Epoch 371/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 87.1909 - mae: 7.9341 - val_loss: 64.0175 - val_mae: 5.9001 - learning_rate: 1.0000e-05\n",
      "Epoch 372/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 87.1727 - mae: 7.9331 - val_loss: 63.9963 - val_mae: 5.8988 - learning_rate: 1.0000e-05\n",
      "Epoch 373/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 87.1542 - mae: 7.9322 - val_loss: 63.9752 - val_mae: 5.8974 - learning_rate: 1.0000e-05\n",
      "Epoch 374/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 87.1364 - mae: 7.9312 - val_loss: 63.9541 - val_mae: 5.8960 - learning_rate: 1.0000e-05\n",
      "Epoch 375/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 87.1180 - mae: 7.9302 - val_loss: 63.9330 - val_mae: 5.8946 - learning_rate: 1.0000e-05\n",
      "Epoch 376/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 87.0996 - mae: 7.9293 - val_loss: 63.9119 - val_mae: 5.8932 - learning_rate: 1.0000e-05\n",
      "Epoch 377/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 87.0813 - mae: 7.9283 - val_loss: 63.8909 - val_mae: 5.8918 - learning_rate: 1.0000e-05\n",
      "Epoch 378/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 87.0630 - mae: 7.9273 - val_loss: 63.8698 - val_mae: 5.8904 - learning_rate: 1.0000e-05\n",
      "Epoch 379/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 87.0448 - mae: 7.9263 - val_loss: 63.8489 - val_mae: 5.8891 - learning_rate: 1.0000e-05\n",
      "Epoch 380/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 87.0265 - mae: 7.9254 - val_loss: 63.8286 - val_mae: 5.8877 - learning_rate: 1.0000e-05\n",
      "Epoch 381/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 87.0082 - mae: 7.9244 - val_loss: 63.8082 - val_mae: 5.8864 - learning_rate: 1.0000e-05\n",
      "Epoch 382/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 86.9901 - mae: 7.9234 - val_loss: 63.7877 - val_mae: 5.8850 - learning_rate: 1.0000e-05\n",
      "Epoch 383/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.9715 - mae: 7.9225 - val_loss: 63.7674 - val_mae: 5.8837 - learning_rate: 1.0000e-05\n",
      "Epoch 384/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.9532 - mae: 7.9215 - val_loss: 63.7469 - val_mae: 5.8823 - learning_rate: 1.0000e-05\n",
      "Epoch 385/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 86.9346 - mae: 7.9205 - val_loss: 63.7266 - val_mae: 5.8810 - learning_rate: 1.0000e-05\n",
      "Epoch 386/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 86.9163 - mae: 7.9195 - val_loss: 63.7061 - val_mae: 5.8796 - learning_rate: 1.0000e-05\n",
      "Epoch 387/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 86.8980 - mae: 7.9186 - val_loss: 63.6857 - val_mae: 5.8783 - learning_rate: 1.0000e-05\n",
      "Epoch 388/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.8796 - mae: 7.9176 - val_loss: 63.6655 - val_mae: 5.8770 - learning_rate: 1.0000e-05\n",
      "Epoch 389/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.8610 - mae: 7.9166 - val_loss: 63.6454 - val_mae: 5.8757 - learning_rate: 1.0000e-05\n",
      "Epoch 390/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.8430 - mae: 7.9156 - val_loss: 63.6253 - val_mae: 5.8744 - learning_rate: 1.0000e-05\n",
      "Epoch 391/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.8239 - mae: 7.9147 - val_loss: 63.6052 - val_mae: 5.8731 - learning_rate: 1.0000e-05\n",
      "Epoch 392/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 86.8059 - mae: 7.9137 - val_loss: 63.5851 - val_mae: 5.8718 - learning_rate: 1.0000e-05\n",
      "Epoch 393/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.7874 - mae: 7.9127 - val_loss: 63.5650 - val_mae: 5.8705 - learning_rate: 1.0000e-05\n",
      "Epoch 394/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 86.7687 - mae: 7.9117 - val_loss: 63.5449 - val_mae: 5.8692 - learning_rate: 1.0000e-05\n",
      "Epoch 395/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.7505 - mae: 7.9108 - val_loss: 63.5248 - val_mae: 5.8679 - learning_rate: 1.0000e-05\n",
      "Epoch 396/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 86.7319 - mae: 7.9098 - val_loss: 63.5047 - val_mae: 5.8666 - learning_rate: 1.0000e-05\n",
      "Epoch 397/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.7134 - mae: 7.9088 - val_loss: 63.4846 - val_mae: 5.8653 - learning_rate: 1.0000e-05\n",
      "Epoch 398/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 86.6951 - mae: 7.9078 - val_loss: 63.4645 - val_mae: 5.8640 - learning_rate: 1.0000e-05\n",
      "Epoch 399/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 86.6762 - mae: 7.9068 - val_loss: 63.4444 - val_mae: 5.8627 - learning_rate: 1.0000e-05\n",
      "Epoch 400/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 86.6579 - mae: 7.9059 - val_loss: 63.4243 - val_mae: 5.8614 - learning_rate: 1.0000e-05\n",
      "Epoch 401/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 86.6394 - mae: 7.9049 - val_loss: 63.4042 - val_mae: 5.8601 - learning_rate: 1.0000e-05\n",
      "Epoch 402/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 86.6208 - mae: 7.9039 - val_loss: 63.3847 - val_mae: 5.8588 - learning_rate: 1.0000e-05\n",
      "Epoch 403/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 86.6024 - mae: 7.9029 - val_loss: 63.3654 - val_mae: 5.8576 - learning_rate: 1.0000e-05\n",
      "Epoch 404/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 86.5838 - mae: 7.9019 - val_loss: 63.3462 - val_mae: 5.8563 - learning_rate: 1.0000e-05\n",
      "Epoch 405/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.5650 - mae: 7.9009 - val_loss: 63.3271 - val_mae: 5.8550 - learning_rate: 1.0000e-05\n",
      "Epoch 406/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.5464 - mae: 7.9000 - val_loss: 63.3079 - val_mae: 5.8538 - learning_rate: 1.0000e-05\n",
      "Epoch 407/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 86.5279 - mae: 7.8990 - val_loss: 63.2888 - val_mae: 5.8525 - learning_rate: 1.0000e-05\n",
      "Epoch 408/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 86.5092 - mae: 7.8980 - val_loss: 63.2696 - val_mae: 5.8513 - learning_rate: 1.0000e-05\n",
      "Epoch 409/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 86.4907 - mae: 7.8970 - val_loss: 63.2505 - val_mae: 5.8500 - learning_rate: 1.0000e-05\n",
      "Epoch 410/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 86.4723 - mae: 7.8960 - val_loss: 63.2313 - val_mae: 5.8487 - learning_rate: 1.0000e-05\n",
      "Epoch 411/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 86.4539 - mae: 7.8950 - val_loss: 63.2121 - val_mae: 5.8475 - learning_rate: 1.0000e-05\n",
      "Epoch 412/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 86.4348 - mae: 7.8940 - val_loss: 63.1930 - val_mae: 5.8462 - learning_rate: 1.0000e-05\n",
      "Epoch 413/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 86.4166 - mae: 7.8931 - val_loss: 63.1738 - val_mae: 5.8449 - learning_rate: 1.0000e-05\n",
      "Epoch 414/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 86.3979 - mae: 7.8921 - val_loss: 63.1546 - val_mae: 5.8437 - learning_rate: 1.0000e-05\n",
      "Epoch 415/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 86.3794 - mae: 7.8911 - val_loss: 63.1354 - val_mae: 5.8424 - learning_rate: 1.0000e-05\n",
      "Epoch 416/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 86.3607 - mae: 7.8901 - val_loss: 63.1162 - val_mae: 5.8412 - learning_rate: 1.0000e-05\n",
      "Epoch 417/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.3418 - mae: 7.8891 - val_loss: 63.0971 - val_mae: 5.8399 - learning_rate: 1.0000e-05\n",
      "Epoch 418/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.3232 - mae: 7.8881 - val_loss: 63.0780 - val_mae: 5.8387 - learning_rate: 1.0000e-05\n",
      "Epoch 419/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 86.3045 - mae: 7.8872 - val_loss: 63.0589 - val_mae: 5.8374 - learning_rate: 1.0000e-05\n",
      "Epoch 420/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.2862 - mae: 7.8862 - val_loss: 63.0398 - val_mae: 5.8362 - learning_rate: 1.0000e-05\n",
      "Epoch 421/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 86.2674 - mae: 7.8852 - val_loss: 63.0207 - val_mae: 5.8349 - learning_rate: 1.0000e-05\n",
      "Epoch 422/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 86.2486 - mae: 7.8842 - val_loss: 63.0016 - val_mae: 5.8336 - learning_rate: 1.0000e-05\n",
      "Epoch 423/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 86.2296 - mae: 7.8832 - val_loss: 62.9825 - val_mae: 5.8324 - learning_rate: 1.0000e-05\n",
      "Epoch 424/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 86.2113 - mae: 7.8822 - val_loss: 62.9634 - val_mae: 5.8311 - learning_rate: 1.0000e-05\n",
      "Epoch 425/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 86.1927 - mae: 7.8813 - val_loss: 62.9443 - val_mae: 5.8298 - learning_rate: 1.0000e-05\n",
      "Epoch 426/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 86.1735 - mae: 7.8803 - val_loss: 62.9252 - val_mae: 5.8286 - learning_rate: 1.0000e-05\n",
      "Epoch 427/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 86.1548 - mae: 7.8793 - val_loss: 62.9061 - val_mae: 5.8273 - learning_rate: 1.0000e-05\n",
      "Epoch 428/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 86.1361 - mae: 7.8783 - val_loss: 62.8870 - val_mae: 5.8260 - learning_rate: 1.0000e-05\n",
      "Epoch 429/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 86.1170 - mae: 7.8773 - val_loss: 62.8679 - val_mae: 5.8248 - learning_rate: 1.0000e-05\n",
      "Epoch 430/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 86.0985 - mae: 7.8763 - val_loss: 62.8488 - val_mae: 5.8235 - learning_rate: 1.0000e-05\n",
      "Epoch 431/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 86.0800 - mae: 7.8753 - val_loss: 62.8296 - val_mae: 5.8222 - learning_rate: 1.0000e-05\n",
      "Epoch 432/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 86.0610 - mae: 7.8743 - val_loss: 62.8104 - val_mae: 5.8209 - learning_rate: 1.0000e-05\n",
      "Epoch 433/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 86.0421 - mae: 7.8733 - val_loss: 62.7914 - val_mae: 5.8197 - learning_rate: 1.0000e-05\n",
      "Epoch 434/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 86.0231 - mae: 7.8723 - val_loss: 62.7722 - val_mae: 5.8184 - learning_rate: 1.0000e-05\n",
      "Epoch 435/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 86.0044 - mae: 7.8713 - val_loss: 62.7531 - val_mae: 5.8171 - learning_rate: 1.0000e-05\n",
      "Epoch 436/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 85.9851 - mae: 7.8703 - val_loss: 62.7341 - val_mae: 5.8159 - learning_rate: 1.0000e-05\n",
      "Epoch 437/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 85.9663 - mae: 7.8693 - val_loss: 62.7150 - val_mae: 5.8146 - learning_rate: 1.0000e-05\n",
      "Epoch 438/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 85.9472 - mae: 7.8683 - val_loss: 62.6960 - val_mae: 5.8133 - learning_rate: 1.0000e-05\n",
      "Epoch 439/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 85.9285 - mae: 7.8673 - val_loss: 62.6769 - val_mae: 5.8120 - learning_rate: 1.0000e-05\n",
      "Epoch 440/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 85.9094 - mae: 7.8663 - val_loss: 62.6578 - val_mae: 5.8108 - learning_rate: 1.0000e-05\n",
      "Epoch 441/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 85.8908 - mae: 7.8653 - val_loss: 62.6387 - val_mae: 5.8095 - learning_rate: 1.0000e-05\n",
      "Epoch 442/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 85.8717 - mae: 7.8643 - val_loss: 62.6196 - val_mae: 5.8082 - learning_rate: 1.0000e-05\n",
      "Epoch 443/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 85.8528 - mae: 7.8633 - val_loss: 62.6006 - val_mae: 5.8069 - learning_rate: 1.0000e-05\n",
      "Epoch 444/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 85.8337 - mae: 7.8623 - val_loss: 62.5816 - val_mae: 5.8057 - learning_rate: 1.0000e-05\n",
      "Epoch 445/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 85.8147 - mae: 7.8613 - val_loss: 62.5625 - val_mae: 5.8044 - learning_rate: 1.0000e-05\n",
      "Epoch 446/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 85.7953 - mae: 7.8603 - val_loss: 62.5434 - val_mae: 5.8031 - learning_rate: 1.0000e-05\n",
      "Epoch 447/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 85.7766 - mae: 7.8593 - val_loss: 62.5244 - val_mae: 5.8018 - learning_rate: 1.0000e-05\n",
      "Epoch 448/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 85.7577 - mae: 7.8583 - val_loss: 62.5053 - val_mae: 5.8005 - learning_rate: 1.0000e-05\n",
      "Epoch 449/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 85.7385 - mae: 7.8572 - val_loss: 62.4863 - val_mae: 5.7992 - learning_rate: 1.0000e-05\n",
      "Epoch 450/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 85.7191 - mae: 7.8562 - val_loss: 62.4673 - val_mae: 5.7980 - learning_rate: 1.0000e-05\n",
      "Epoch 451/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 85.7006 - mae: 7.8552 - val_loss: 62.4483 - val_mae: 5.7967 - learning_rate: 1.0000e-05\n",
      "Epoch 452/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 85.6813 - mae: 7.8542 - val_loss: 62.4292 - val_mae: 5.7954 - learning_rate: 1.0000e-05\n",
      "Epoch 453/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 85.6621 - mae: 7.8532 - val_loss: 62.4102 - val_mae: 5.7941 - learning_rate: 1.0000e-05\n",
      "Epoch 454/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 85.6430 - mae: 7.8522 - val_loss: 62.3912 - val_mae: 5.7928 - learning_rate: 1.0000e-05\n",
      "Epoch 455/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 85.6238 - mae: 7.8512 - val_loss: 62.3722 - val_mae: 5.7915 - learning_rate: 1.0000e-05\n",
      "Epoch 456/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 85.6052 - mae: 7.8502 - val_loss: 62.3532 - val_mae: 5.7902 - learning_rate: 1.0000e-05\n",
      "Epoch 457/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 85.5861 - mae: 7.8492 - val_loss: 62.3342 - val_mae: 5.7889 - learning_rate: 1.0000e-05\n",
      "Epoch 458/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 85.5666 - mae: 7.8481 - val_loss: 62.3154 - val_mae: 5.7877 - learning_rate: 1.0000e-05\n",
      "Epoch 459/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 85.5472 - mae: 7.8471 - val_loss: 62.2965 - val_mae: 5.7864 - learning_rate: 1.0000e-05\n",
      "Epoch 460/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 85.5284 - mae: 7.8461 - val_loss: 62.2775 - val_mae: 5.7851 - learning_rate: 1.0000e-05\n",
      "Epoch 461/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 85.5095 - mae: 7.8451 - val_loss: 62.2586 - val_mae: 5.7838 - learning_rate: 1.0000e-05\n",
      "Epoch 462/1000\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 85.4902 - mae: 7.8440 - val_loss: 62.2396 - val_mae: 5.7825 - learning_rate: 1.0000e-05\n",
      "Epoch 463/1000\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 85.9442 - mae: 7.8686"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=25, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=10, verbose=1,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=1000,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b050dc-8b49-45f7-bfd0-9e5acf45ca19",
   "metadata": {
    "id": "e5b050dc-8b49-45f7-bfd0-9e5acf45ca19"
   },
   "outputs": [],
   "source": [
    "# --- 1) Collect validation data into arrays ---\n",
    "X_test, y_test = [], []\n",
    "for X_batch, y_batch in val_dataset:\n",
    "    X_test.append(X_batch.numpy())\n",
    "    y_test.append(y_batch.numpy())\n",
    "\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "\n",
    "print(\"X_test shape:\", X_test.shape)  # (num_test, window_size, 10)\n",
    "print(\"y_test shape:\", y_test.shape)  # (num_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8394a85-835a-4395-8ebf-26c2377d7387",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f8394a85-835a-4395-8ebf-26c2377d7387",
    "outputId": "3b4618f6-91da-4369-e856-31960920d433"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)  # shape: (num_test, 9)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(df_en.columns):\n",
    "    mae = np.mean(np.abs(y_test[:, i] - y_pred[:, i]))\n",
    "    # Compute R² (fixed parentheses)\n",
    "    ss_res = np.sum((y_test[:, i] - y_pred[:, i]) ** 2)\n",
    "    ss_tot = np.sum((y_test[:, i] - np.mean(y_test[:, i])) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot if ss_tot > 0 else np.nan\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.plot(y_test[:, i], label='Actual')\n",
    "    plt.plot(y_pred[:, i], label='Predicted')\n",
    "    plt.title(f\"{col}\\nMAE: {mae:.3f} | R²: {r2:.3f}\")\n",
    "    plt.xlabel('Time step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c87f20-1821-4e42-aa49-8638cb73edae",
   "metadata": {
    "id": "a5c87f20-1821-4e42-aa49-8638cb73edae"
   },
   "outputs": [],
   "source": [
    "np.sum(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb423f5-b81e-45d9-b199-b3c1d461b494",
   "metadata": {
    "id": "2cb423f5-b81e-45d9-b199-b3c1d461b494"
   },
   "outputs": [],
   "source": [
    "last_window = df_en.values[-window_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c9030a-9fb3-4d31-98ec-f7ea7ef67736",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71011d4-9e6d-4d90-ac8a-c8332d084fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_future(model, last_window, steps):\n",
    "    \"\"\"\n",
    "    Predict future steps recursively using the trained model.\n",
    "    model: trained keras model\n",
    "    last_window: np.array (window_size, n_features)\n",
    "    steps: number of timesteps to predict\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    window = last_window.copy()\n",
    "\n",
    "    for step in range(steps):\n",
    "        # Expand to batch size 1 (same shape as during training)\n",
    "        x_input = np.expand_dims(window, axis=0)  # shape (1, window_size, n_features)\n",
    "\n",
    "        # Model prediction\n",
    "        y_pred = model.predict(x_input, verbose=0)[0]  # (n_features,)\n",
    "        preds.append(y_pred)\n",
    "\n",
    "        # Slide window forward: drop oldest row, append prediction\n",
    "        window = np.vstack([window[1:], y_pred])\n",
    "\n",
    "    return np.array(preds)  # shape (steps, n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420065cf-cb8d-4faf-ad60-3df54b1c13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_steps = 12\n",
    "future_preds = forecast_future(model, last_window, steps=future_steps)\n",
    "\n",
    "print(\"Shape:\", future_preds.shape)\n",
    "# => (12, n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e319d7f-26fd-4536-8c72-a2fb9a01f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "backtest_window = df_en.values[:split][-window_size:]\n",
    "y_true = df_en.values[split:split+len(val_data)]  # true values for 2024\n",
    "\n",
    "y_pred = forecast_future(model, backtest_window, steps=len(val_data))\n",
    "\n",
    "# Compare with real 2024 data\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"Backtest MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17710b73-2982-42b7-9abc-276615423b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Step 1: Backtest ---\n",
    "backtest_window = df_en.values[:split][-window_size:]\n",
    "y_test = df_en.values[split:]  # true values for 2024\n",
    "y_pred_backtest = forecast_future(model, backtest_window, steps=len(y_test))\n",
    "\n",
    "# --- Step 2: Future Forecast ---\n",
    "last_window = df_en.values[-window_size:]\n",
    "future_steps = 12\n",
    "y_pred_future = forecast_future(model, last_window, steps=future_steps)\n",
    "\n",
    "# --- Step 3: Plotting ---\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(df_en.columns):\n",
    "    # Compute metrics for backtest\n",
    "    mae = np.mean(np.abs(y_test[:, i] - y_pred_backtest[:, i]))\n",
    "    ss_res = np.sum((y_test[:, i] - y_pred_backtest[:, i]) ** 2)\n",
    "    ss_tot = np.sum((y_test[:, i] - np.mean(y_test[:, i])) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot if ss_tot > 0 else np.nan\n",
    "\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "\n",
    "    # Plot actual (2024)\n",
    "    plt.plot(y_test[:, i], label=\"Actual 2024\", color=\"black\")\n",
    "\n",
    "    # Plot backtest predictions\n",
    "    plt.plot(y_pred_backtest[:, i], label=\"Predicted 2024\", linestyle=\"--\", color=\"tab:blue\")\n",
    "\n",
    "    # Plot future predictions (2025+)\n",
    "    future_x = np.arange(len(y_test), len(y_test) + future_steps)\n",
    "    plt.plot(future_x, y_pred_future[:, i], label=\"Predicted 2025+\", linestyle=\":\", color=\"tab:red\")\n",
    "\n",
    "    plt.title(f\"{col}\\nMAE: {mae:.3f} | R²: {r2:.3f}\")\n",
    "    plt.xlabel(\"Time step\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
