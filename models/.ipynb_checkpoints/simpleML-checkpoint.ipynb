{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44061432-c75b-441c-9465-6b561e857a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.mlemodel import MLEModel\n",
    "from statsmodels.tsa.statespace.dynamic_factor import DynamicFactor\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen, VECM\n",
    "from statsmodels.tsa.statespace.mlemodel import MLEModel\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9746686e-269b-4dc9-a381-230335e8dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laste poll-of-polls data\n",
    "url = \"https://raw.githubusercontent.com/jensmorten/onesixtynine/main/data/pollofpolls_master.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4dd0a9-6e00-4cd8-ab75-849c90eee3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime and set the date to the end of the month\n",
    "df[\"Mnd\"] = pd.to_datetime(df[\"Mnd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b450fc8-8f09-414e-a2ba-2e8b671becd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2008-01-31', '2008-02-29', '2008-03-31', '2008-04-30',\n",
       "               '2008-05-31', '2008-06-30', '2008-07-31', '2008-08-31',\n",
       "               '2008-09-30', '2008-10-31',\n",
       "               ...\n",
       "               '2025-01-31', '2025-02-28', '2025-03-31', '2025-04-30',\n",
       "               '2025-05-31', '2025-06-30', '2025-07-31', '2025-08-31',\n",
       "               '2025-09-30', '2025-10-31'],\n",
       "              dtype='datetime64[ns]', name='Mnd', length=214, freq='ME')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort values and set index\n",
    "df = df.sort_values(\"Mnd\")\n",
    "df.set_index(\"Mnd\", inplace=True)\n",
    "df.index.to_period('M').to_timestamp('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a65f1a-5a84-44c3-8375-b352dc6e039a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2025-06-30', '2025-07-31', '2025-08-31', '2025-09-30',\n",
      "               '2025-10-31'],\n",
      "              dtype='datetime64[ns]', name='Mnd', freq=None)\n"
     ]
    }
   ],
   "source": [
    "print(df.index[-5:])  # check last few dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13528347-af8f-4e6c-8556-dd06a3e3e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['Ap', 'Hoyre', 'Frp', 'SV', 'SP', 'KrF', 'Venstre', 'MDG', 'Rodt','Andre']]\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "239412db-715c-4512-8df6-dcfac9347385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.api import VAR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def hybrid_var_xgb_with_var_intervals(\n",
    "    df,\n",
    "    n_months=12,\n",
    "    var_lags=6,\n",
    "    lags_ML=12,\n",
    "    random_state=123\n",
    "):\n",
    "    \"\"\"\n",
    "    Hybrid VAR + XGBoost-residuals.\n",
    "    - Uncertainty from VAR forecast_interval\n",
    "    - ML only adjusts the mean (and shifts the whole interval).\n",
    "\n",
    "    Returns:\n",
    "        forecast_df, forecast_lower_df, forecast_upper_df\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1. Fit VAR and get intervals\n",
    "    # -------------------------------\n",
    "    var_model = VAR(df)\n",
    "    var_res = var_model.fit(maxlags=6)\n",
    "\n",
    "    mean_var, lower_var, upper_var = var_res.forecast_interval(\n",
    "        df.values[-var_res.k_ar:], steps=n_months\n",
    "    )\n",
    "\n",
    "    mean_var_df = pd.DataFrame(mean_var, columns=df.columns)\n",
    "    lower_var_df = pd.DataFrame(lower_var, columns=df.columns)\n",
    "    upper_var_df = pd.DataFrame(upper_var, columns=df.columns)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. Build residual dataset for ML\n",
    "    # -------------------------------\n",
    "    # VAR fitted values & residuals\n",
    "    var_fitted = var_res.fittedvalues\n",
    "    aligned_true = df.iloc[var_res.k_ar:]          # same index as fittedvalues\n",
    "    resid = aligned_true.values - var_fitted.values\n",
    "    resid_df = pd.DataFrame(resid, index=aligned_true.index, columns=df.columns)\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for t in range(lags_ML, len(df)):\n",
    "        # lags_ML months of all parties as features\n",
    "        X.append(df.iloc[t-lags_ML:t].values.flatten())\n",
    "        # residual at time t (aligned with residual_df via index)\n",
    "        y.append(resid_df.iloc[t - var_res.k_ar].values)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 3. Fit XGBoost on residuals\n",
    "    # -------------------------------\n",
    "    split = int(0.8 * len(X))\n",
    "    X_tr, X_va = X[:split], X[split:]\n",
    "    y_tr, y_va = y[:split], y[split:]\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=random_state,\n",
    "        eval_metric=\"rmse\"   # <-- THIS IS REQUIRED\n",
    "    )\n",
    "    \n",
    "    xgb.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # -------------------------------\n",
    "    # 4. Predict future residuals recursively\n",
    "    # -------------------------------\n",
    "    current_win = df.values[-lags_ML:].copy()\n",
    "    pred_resid_list = []\n",
    "\n",
    "    for i in range(n_months):\n",
    "        Xi = current_win.flatten().reshape(1, -1)\n",
    "        pred = xgb.predict(Xi)[0]\n",
    "        pred_resid_list.append(pred)\n",
    "\n",
    "        # advance window using VAR mean forecast (not true values)\n",
    "        var_step_mean = mean_var_df.iloc[i].values\n",
    "        current_win = np.vstack([current_win[1:], var_step_mean])\n",
    "\n",
    "    ml_resid_df = pd.DataFrame(pred_resid_list, columns=df.columns)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 5. Build hybrid forecast + intervals\n",
    "    # -------------------------------\n",
    "    # VAR intervals, shifted by ML residual prediction (same width)\n",
    "    forecast_index = pd.date_range(\n",
    "        start=df.index[-1] + pd.offsets.MonthEnd(1),\n",
    "        periods=n_months,\n",
    "        freq=\"M\"\n",
    "    )\n",
    "\n",
    "    forecast_df = mean_var_df + ml_resid_df\n",
    "    forecast_lower_df = lower_var_df + ml_resid_df\n",
    "    forecast_upper_df = upper_var_df + ml_resid_df\n",
    "\n",
    "    forecast_df.index = forecast_index\n",
    "    forecast_lower_df.index = forecast_index\n",
    "    forecast_upper_df.index = forecast_index\n",
    "\n",
    "    return forecast_df, forecast_lower_df, forecast_upper_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eea36f5-a70c-4009-aca8-ac0d4341156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jens.nilsen\\python\\WPy64-31230\\python-3.12.3.amd64\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency ME will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m forecast_df, forecast_lower_df, forecast_upper_df \u001b[38;5;241m=\u001b[39m \u001b[43mhybrid_var_xgb_with_var_intervals\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_months\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvar_lags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlags_ML\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 72\u001b[0m, in \u001b[0;36mhybrid_var_xgb_with_var_intervals\u001b[1;34m(df, n_months, var_lags, lags_ML, random_state)\u001b[0m\n\u001b[0;32m     61\u001b[0m y_tr, y_va \u001b[38;5;241m=\u001b[39m y[:split], y[split:]\n\u001b[0;32m     62\u001b[0m xgb \u001b[38;5;241m=\u001b[39m XGBRegressor(\n\u001b[0;32m     63\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m     64\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m )\n\u001b[1;32m---> 72\u001b[0m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_va\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_va\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     77\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# 4. Predict future residuals recursively\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m     82\u001b[0m current_win \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m-\u001b[39mlags_ML:]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\python\\WPy64-31230\\python-3.12.3.amd64\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "forecast_df, forecast_lower_df, forecast_upper_df = hybrid_var_xgb_with_var_intervals(\n",
    "    df,\n",
    "    n_months=12,\n",
    "    var_lags=6,\n",
    "    lags_ML=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0de008-eb91-4232-890a-532965784d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define colors for consistency ---\n",
    "colors = {\n",
    "    'Ap': '#FF0000',        # Red\n",
    "    'Hoyre': '#0000FF',     # Blue\n",
    "    'Frp': '#00008B',       # Dark Blue\n",
    "    'SV': '#FF6347',        # Light Red (Tomato)\n",
    "    'SP': '#006400',        # Dark Green\n",
    "    'KrF': '#FFD700',       # Yellow (Gold)\n",
    "    'Venstre': '#ADD8E6',   # Light Blue\n",
    "    'MDG': '#008000',       # Green\n",
    "    'Rodt': '#8B0000',      # Dark Red\n",
    "    'Andre': '#808080'      # Gray\n",
    "}\n",
    "\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "months_back = 12\n",
    "df_recent = df.iloc[-months_back:]  # last 12 months actual\n",
    "\n",
    "for party, color in colors.items():\n",
    "\n",
    "    # Actual observed data\n",
    "    plt.plot(\n",
    "        df_recent.index,\n",
    "        df_recent[party],\n",
    "        marker=\"o\",\n",
    "        color=color,\n",
    "        label=f\"{party}\"\n",
    "    )\n",
    "\n",
    "    # Forecast line (bootstrap mean)\n",
    "    plt.plot(\n",
    "        forecast_df.index,\n",
    "        forecast_df[party],\n",
    "        linestyle=\"dashed\",\n",
    "        color=color\n",
    "    )\n",
    "\n",
    "    # Connect last actual point to first forecast\n",
    "    plt.plot(\n",
    "        [df_recent.index[-1], forecast_df.index[0]],\n",
    "        [df_recent[party].iloc[-1], forecast_df[party].iloc[0]],\n",
    "        linestyle=\"dashed\",\n",
    "        color=color\n",
    "    )\n",
    "\n",
    "    # Confidence interval (bootstrap)\n",
    "    plt.fill_between(\n",
    "        forecast_df.index,\n",
    "        forecast_lower_df[party],\n",
    "        forecast_upper_df[party],\n",
    "        color=color,\n",
    "        alpha=0.15\n",
    "    )\n",
    "\n",
    "# --- Vertical monthly guides ---\n",
    "dates = pd.date_range(start=df_recent.index[0], end=forecast_df.index[-1], freq=\"MS\")\n",
    "for date in dates:\n",
    "    plt.axvline(date, color=\"gray\", linestyle=\"dotted\", alpha=0.3)\n",
    "\n",
    "# --- Horizontal percentage lines ---\n",
    "for percent in range(0, 45, 5):\n",
    "    plt.axhline(percent, color=\"gray\", linestyle=\"dotted\", alpha=0.3)\n",
    "\n",
    "# --- Final formatting ---\n",
    "plt.xlim(df_recent.index[0], forecast_df.index[-1])\n",
    "plt.ylim(0, 40)\n",
    "plt.xlabel(\"Tid\")\n",
    "plt.ylabel(\"Prosent oppslutning\")\n",
    "plt.title(\"OneSixtyNine\")\n",
    "plt.legend(loc=\"upper left\", ncol=2)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7f5d5-f8b1-4ac2-932b-ed0037720f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def tune_hybrid_params(\n",
    "    df,\n",
    "    horizon=6,\n",
    "    lags_var_list=[5,6,7],\n",
    "    lags_ml_list=[10,12,14],\n",
    "    depth_list=[4,5,6],\n",
    "    lr_list=[0.03, 0.05],\n",
    "    n_estimators_list=[250,750],\n",
    "):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for l_var, l_ml, depth, lr, n_est in product(\n",
    "        lags_var_list, lags_ml_list, depth_list, lr_list,  n_estimators_list\n",
    "    ):\n",
    "        err = []\n",
    "        step = max(1, horizon * 6)\n",
    "        for t in range(60, len(df) - horizon, step):\n",
    "            train = df.iloc[:t]\n",
    "\n",
    "            # --- VAR ---\n",
    "            var_res = VAR(train).fit(maxlags=l_var)\n",
    "\n",
    "            mean_var, _, _ = var_res.forecast_interval(\n",
    "                train.values[-var_res.k_ar:], steps=horizon\n",
    "            )\n",
    "\n",
    "            var_pred = mean_var[-1]\n",
    "\n",
    "            # --- Residual ML ---\n",
    "            resid = train.iloc[var_res.k_ar:].values - var_res.fittedvalues.values\n",
    "\n",
    "            X, y = [], []\n",
    "            for i in range(l_ml, len(train)):\n",
    "                if i - var_res.k_ar < 0:\n",
    "                    continue\n",
    "                X.append(train.iloc[i-l_ml:i].values.flatten())\n",
    "                y.append(resid[i - var_res.k_ar])\n",
    "\n",
    "            if len(X) < 50:\n",
    "                continue\n",
    "\n",
    "            X = np.array(X)\n",
    "            y = np.array(y)\n",
    "\n",
    "            xgb = XGBRegressor(\n",
    "                n_estimators=n_est,\n",
    "                max_depth=depth,\n",
    "                learning_rate=lr,\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.9,\n",
    "                objective=\"reg:squarederror\",\n",
    "                verbosity=0\n",
    "            )\n",
    "            xgb.fit(X, y)\n",
    "\n",
    "            # predict residual\n",
    "            win = train.values[-l_ml:].copy()\n",
    "            pred_resid = xgb.predict(win.flatten().reshape(1, -1))[0]\n",
    "\n",
    "            hybrid_pred = var_pred + pred_resid\n",
    "            true = df.iloc[t + horizon - 1].values\n",
    "\n",
    "            err.append(np.mean(np.abs(true - hybrid_pred)))\n",
    "\n",
    "        if err:\n",
    "            results.append({\n",
    "                \"lags_var\": l_var,\n",
    "                \"lags_ml\": l_ml,\n",
    "                \"max_depth\": depth,\n",
    "                \"learning_rate\": lr,\n",
    "                \"estimators\": lr,\n",
    "                \"MAE\": np.mean(err)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(\"MAE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156cbb73-ad71-4223-8def-5dab08c99e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results = tune_hybrid_params(df)\n",
    "tuning_results.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
